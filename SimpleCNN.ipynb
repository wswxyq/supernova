{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from os import listdir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'fd-1kpc-9.6sm-0-overlaid.csv/npy0'\n",
    "file_list = listdir(data_path)\n",
    "file_list.sort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siglist=[]\n",
    "for items in file_list:\n",
    "    data = np.load( os.path.join( data_path, items ) )\n",
    "    siglist.append(data['sig'])\n",
    "\n",
    "sigindex=np.array(file_list, dtype='U')[np.array(siglist, dtype=int)==1]\n",
    "bkgindex=np.array(file_list, dtype='U')[np.array(siglist, dtype=int)==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_path_, file_list_):\n",
    "        self.data_path = data_path_\n",
    "        self.file_list = file_list_\n",
    "        self.len = len(file_list_)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        file_name = self.file_list[index]\n",
    "        data = np.load( os.path.join( self.data_path, file_name ) )\n",
    "        return torch.from_numpy(data['imxz'][None, :, :]).to(torch.float)/4096, torch.from_numpy(data['imyz'][None, :, :]).to(torch.float)/4096, torch.from_numpy(data['sig']).to(torch.long)\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "mydataset = MyDataset(data_path, file_list)\n",
    "batch_size_train = 6\n",
    "batch_size_test = 4\n",
    "\n",
    "\n",
    "train_size = int(0.8 * len(mydataset))\n",
    "test_size = len(mydataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(mydataset, [train_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size_train,\n",
    "                                            shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size_test,\n",
    "                                            shuffle=False)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1_1 = nn.Conv2d(1, 64, 5) \n",
    "        self.conv1_2 = nn.Conv2d(1, 64, 5) \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2_1 = nn.Conv2d(64, 16, 5)\n",
    "        self.conv2_2 = nn.Conv2d(64, 16, 5)\n",
    "        self.fc1_1 = nn.Linear(16 * 221 * 93, 10)\n",
    "        self.fc1_2 = nn.Linear(16 * 221 * 93, 10)\n",
    "        self.fc2= nn.Linear(20, 2)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # x1, x2 shape: (896, 384)      channel = 1\n",
    "        x1 = self.pool(F.relu(self.conv1_1(x1))) # shape: (896, 384)->(892, 380)->(446, 190)\n",
    "        x1 = self.pool(F.relu(self.conv2_1(x1))) # shape: (446, 190)->(442, 186)->(221, 93)\n",
    "        x1 = torch.flatten(x1, 1) # flatten all dimensions except batch \n",
    "        x1 = F.relu(self.fc1_1(x1))\n",
    "\n",
    "        x2 = self.pool(F.relu(self.conv1_2(x2))) # shape: (896, 384)->(892, 380)->(446, 190)\n",
    "        x2 = self.pool(F.relu(self.conv2_2(x2))) # shape: (446, 190)->(442, 186)->(221, 93)\n",
    "        x2 = torch.flatten(x2, 1) # flatten all dimensions except batch \n",
    "        x2 = F.relu(self.fc1_2(x2))\n",
    "\n",
    "        return self.fc2(torch.cat((x1, x2), 1))\n",
    "net=Net().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6631582"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce GTX 1660 Ti'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, batch: 0 Loss: 0.1207\n",
      "Epoch: 0, batch: 1 Loss: 0.8053\n",
      "Epoch: 0, batch: 2 Loss: 0.4629\n",
      "Epoch: 0, batch: 3 Loss: 0.4628\n",
      "Epoch: 0, batch: 4 Loss: 0.1211\n",
      "Epoch: 0, batch: 5 Loss: 0.1211\n",
      "Epoch: 0, batch: 6 Loss: 0.1211\n",
      "Epoch: 0, batch: 7 Loss: 0.4628\n",
      "Epoch: 0, batch: 8 Loss: 0.4628\n",
      "Epoch: 0, batch: 9 Loss: 0.1209\n",
      "Epoch: 0, batch: 10 Loss: 0.1208\n",
      "Epoch: 0, batch: 11 Loss: 0.4630\n",
      "Epoch: 0, batch: 12 Loss: 0.4630\n",
      "Epoch: 0, batch: 13 Loss: 0.8056\n",
      "Epoch: 0, batch: 14 Loss: 0.4630\n",
      "Epoch: 0, batch: 15 Loss: 0.8051\n",
      "Epoch: 0, batch: 16 Loss: 0.4628\n",
      "Epoch: 0, batch: 17 Loss: 1.1454\n",
      "Epoch: 0, batch: 18 Loss: 0.1219\n",
      "Epoch: 0, batch: 19 Loss: 0.4622\n",
      "Epoch: 0, batch: 20 Loss: 0.4621\n",
      "Epoch: 0, batch: 21 Loss: 0.8007\n",
      "Epoch: 0, batch: 22 Loss: 0.1236\n",
      "Epoch: 0, batch: 23 Loss: 0.1240\n",
      "Epoch: 0, batch: 24 Loss: 0.1243\n",
      "Epoch: 0, batch: 25 Loss: 0.4613\n",
      "Epoch: 0, batch: 26 Loss: 0.1246\n",
      "Epoch: 0, batch: 27 Loss: 0.1247\n",
      "Epoch: 0, batch: 28 Loss: 0.1246\n",
      "Epoch: 0, batch: 29 Loss: 0.7980\n",
      "Epoch: 0, batch: 30 Loss: 0.1246\n",
      "Epoch: 0, batch: 31 Loss: 1.1346\n",
      "Epoch: 0, batch: 32 Loss: 0.1248\n",
      "Epoch: 0, batch: 33 Loss: 0.4611\n",
      "Epoch: 0, batch: 34 Loss: 0.1251\n",
      "Epoch: 0, batch: 35 Loss: 0.1252\n",
      "Epoch: 0, batch: 36 Loss: 0.1251\n",
      "Epoch: 0, batch: 37 Loss: 0.1250\n",
      "Epoch: 0, batch: 38 Loss: 0.1248\n",
      "Epoch: 0, batch: 39 Loss: 0.4612\n",
      "Epoch: 0, batch: 40 Loss: 0.1244\n",
      "Epoch: 0, batch: 41 Loss: 0.1241\n",
      "Epoch: 0, batch: 42 Loss: 0.7993\n",
      "Epoch: 0, batch: 43 Loss: 0.4616\n",
      "Epoch: 0, batch: 44 Loss: 0.4617\n",
      "Epoch: 0, batch: 45 Loss: 0.4617\n",
      "Epoch: 0, batch: 46 Loss: 0.1236\n",
      "Epoch: 0, batch: 47 Loss: 0.4617\n",
      "Epoch: 0, batch: 48 Loss: 0.1234\n",
      "Epoch: 0, batch: 49 Loss: 0.4618\n",
      "Epoch: 0, batch: 50 Loss: 0.1233\n",
      "Epoch: 0, batch: 51 Loss: 0.4619\n",
      "Epoch: 0, batch: 52 Loss: 0.4619\n",
      "Epoch: 0, batch: 53 Loss: 0.1230\n",
      "Epoch: 0, batch: 54 Loss: 0.8011\n",
      "Epoch: 0, batch: 55 Loss: 0.4620\n",
      "Epoch: 0, batch: 56 Loss: 1.1398\n",
      "Epoch: 0, batch: 57 Loss: 0.1233\n",
      "Epoch: 0, batch: 58 Loss: 0.1236\n",
      "Epoch: 0, batch: 59 Loss: 0.1237\n",
      "Epoch: 0, batch: 60 Loss: 0.4616\n",
      "Epoch: 0, batch: 61 Loss: 0.4616\n",
      "Epoch: 0, batch: 62 Loss: 0.4615\n",
      "Epoch: 0, batch: 63 Loss: 0.1240\n",
      "Epoch: 0, batch: 64 Loss: 0.1240\n",
      "Epoch: 0, batch: 65 Loss: 0.7991\n",
      "Epoch: 0, batch: 66 Loss: 0.4615\n",
      "Epoch: 0, batch: 67 Loss: 0.1242\n",
      "Epoch: 0, batch: 68 Loss: 0.1242\n",
      "Epoch: 0, batch: 69 Loss: 1.1360\n",
      "Epoch: 0, batch: 70 Loss: 0.1244\n",
      "Epoch: 0, batch: 71 Loss: 0.1245\n",
      "Epoch: 0, batch: 72 Loss: 0.1245\n",
      "Epoch: 0, batch: 73 Loss: 0.4613\n",
      "Epoch: 0, batch: 74 Loss: 0.1244\n",
      "Epoch: 0, batch: 75 Loss: 0.4613\n",
      "Epoch: 0, batch: 76 Loss: 0.4614\n",
      "Epoch: 0, batch: 77 Loss: 0.1242\n",
      "Epoch: 0, batch: 78 Loss: 0.4614\n",
      "Epoch: 0, batch: 79 Loss: 0.1241\n",
      "Epoch: 0, batch: 80 Loss: 0.1239\n",
      "Epoch: 0, batch: 81 Loss: 0.1237\n",
      "Epoch: 0, batch: 82 Loss: 0.1234\n",
      "Epoch: 0, batch: 83 Loss: 0.4619\n",
      "Epoch: 0, batch: 84 Loss: 0.1228\n",
      "Epoch: 0, batch: 85 Loss: 0.4621\n",
      "Epoch: 0, batch: 86 Loss: 0.4622\n",
      "Epoch: 0, batch: 87 Loss: 0.4623\n",
      "Epoch: 0, batch: 88 Loss: 0.1220\n",
      "Epoch: 0, batch: 89 Loss: 0.1218\n",
      "Epoch: 0, batch: 90 Loss: 0.4626\n",
      "Epoch: 0, batch: 91 Loss: 0.4627\n",
      "Epoch: 0, batch: 92 Loss: 0.1211\n",
      "Epoch: 0, batch: 93 Loss: 0.1209\n",
      "Epoch: 0, batch: 94 Loss: 0.4630\n",
      "Epoch: 0, batch: 95 Loss: 0.4631\n",
      "Epoch: 0, batch: 96 Loss: 0.4631\n",
      "Epoch: 0, batch: 97 Loss: 0.1202\n",
      "Epoch: 0, batch: 98 Loss: 0.1200\n",
      "Epoch: 0, batch: 99 Loss: 0.1198\n",
      "Epoch: 0, batch: 100 Loss: 0.4635\n",
      "Epoch: 0, batch: 101 Loss: 0.1193\n",
      "Epoch: 0, batch: 102 Loss: 0.1190\n",
      "Epoch: 0, batch: 103 Loss: 0.4639\n",
      "Epoch: 0, batch: 104 Loss: 0.1184\n",
      "Epoch: 0, batch: 105 Loss: 0.4642\n",
      "Epoch: 0, batch: 106 Loss: 0.4643\n",
      "Epoch: 0, batch: 107 Loss: 0.1177\n",
      "Epoch: 0, batch: 108 Loss: 0.1174\n",
      "Epoch: 0, batch: 109 Loss: 0.4647\n",
      "Epoch: 0, batch: 110 Loss: 0.1169\n",
      "Epoch: 0, batch: 111 Loss: 0.4650\n",
      "Epoch: 0, batch: 112 Loss: 0.1164\n",
      "Epoch: 0, batch: 113 Loss: 0.1161\n",
      "Epoch: 0, batch: 114 Loss: 0.8150\n",
      "Epoch: 0, batch: 115 Loss: 0.1157\n",
      "Epoch: 0, batch: 116 Loss: 0.4656\n",
      "Epoch: 0, batch: 117 Loss: 0.4656\n",
      "Epoch: 0, batch: 118 Loss: 0.1152\n",
      "Epoch: 0, batch: 119 Loss: 0.4657\n",
      "Epoch: 0, batch: 120 Loss: 0.4658\n",
      "Epoch: 0, batch: 121 Loss: 0.4658\n",
      "Epoch: 0, batch: 122 Loss: 1.1675\n",
      "Epoch: 0, batch: 123 Loss: 0.4657\n",
      "Epoch: 0, batch: 124 Loss: 0.1156\n",
      "Epoch: 0, batch: 125 Loss: 0.8151\n",
      "Epoch: 0, batch: 126 Loss: 0.1161\n",
      "Epoch: 0, batch: 127 Loss: 1.1627\n",
      "Epoch: 0, batch: 128 Loss: 0.4649\n",
      "Epoch: 0, batch: 129 Loss: 0.4646\n",
      "Epoch: 0, batch: 130 Loss: 0.4644\n",
      "Epoch: 0, batch: 131 Loss: 0.4641\n",
      "Epoch: 0, batch: 132 Loss: 0.8091\n",
      "Epoch: 0, batch: 133 Loss: 0.1193\n",
      "Epoch: 0, batch: 134 Loss: 0.4634\n",
      "Epoch: 0, batch: 135 Loss: 0.4632\n",
      "Epoch: 0, batch: 136 Loss: 0.4630\n",
      "Epoch: 0, batch: 137 Loss: 0.1210\n",
      "Epoch: 0, batch: 138 Loss: 0.1213\n",
      "Epoch: 0, batch: 139 Loss: 0.4626\n",
      "Epoch: 0, batch: 140 Loss: 0.4625\n",
      "Epoch: 0, batch: 141 Loss: 0.4624\n",
      "Epoch: 0, batch: 142 Loss: 1.1424\n",
      "Epoch: 0, batch: 143 Loss: 0.1227\n",
      "Epoch: 0, batch: 144 Loss: 0.4619\n",
      "Epoch: 0, batch: 145 Loss: 1.1383\n",
      "Epoch: 0, batch: 146 Loss: 0.1241\n",
      "Epoch: 0, batch: 147 Loss: 0.1245\n",
      "Epoch: 0, batch: 148 Loss: 0.1249\n",
      "Epoch: 0, batch: 149 Loss: 0.4610\n",
      "Epoch: 0, batch: 150 Loss: 0.7965\n",
      "Epoch: 0, batch: 151 Loss: 0.1257\n",
      "Epoch: 0, batch: 152 Loss: 0.4607\n",
      "Epoch: 0, batch: 153 Loss: 0.4606\n",
      "Epoch: 0, batch: 154 Loss: 0.1265\n",
      "Epoch: 0, batch: 155 Loss: 0.1266\n",
      "Epoch: 0, batch: 156 Loss: 0.1267\n",
      "Epoch: 0, batch: 157 Loss: 0.1266\n",
      "Epoch: 0, batch: 158 Loss: 0.7944\n",
      "Epoch: 0, batch: 159 Loss: 0.1265\n",
      "Epoch: 0, batch: 160 Loss: 0.7944\n",
      "Epoch: 0, batch: 161 Loss: 0.1266\n",
      "Epoch: 0, batch: 162 Loss: 0.4604\n",
      "Epoch: 0, batch: 163 Loss: 0.7940\n",
      "Epoch: 0, batch: 164 Loss: 0.4603\n",
      "Epoch: 0, batch: 165 Loss: 0.1271\n",
      "Epoch: 0, batch: 166 Loss: 0.1272\n",
      "Epoch: 0, batch: 167 Loss: 0.4601\n",
      "Epoch: 0, batch: 168 Loss: 0.4601\n",
      "Epoch: 0, batch: 169 Loss: 0.4601\n",
      "Epoch: 0, batch: 170 Loss: 0.7927\n",
      "Epoch: 0, batch: 171 Loss: 0.7922\n",
      "Epoch: 0, batch: 172 Loss: 0.1281\n",
      "Epoch: 0, batch: 173 Loss: 0.1284\n",
      "Epoch: 0, batch: 174 Loss: 0.1285\n",
      "Epoch: 0, batch: 175 Loss: 0.7907\n",
      "Epoch: 0, batch: 176 Loss: 0.1287\n",
      "Epoch: 0, batch: 177 Loss: 0.4595\n",
      "Epoch: 0, batch: 178 Loss: 0.1289\n",
      "Epoch: 0, batch: 179 Loss: 0.4595\n",
      "Epoch: 0, batch: 180 Loss: 0.4595\n",
      "Epoch: 0, batch: 181 Loss: 1.1202\n",
      "Epoch: 0, batch: 182 Loss: 0.4593\n",
      "Epoch: 0, batch: 183 Loss: 0.4592\n",
      "Epoch: 0, batch: 184 Loss: 0.1301\n",
      "Epoch: 0, batch: 185 Loss: 0.1303\n",
      "Epoch: 0, batch: 186 Loss: 0.1304\n",
      "Epoch: 0, batch: 187 Loss: 0.4589\n",
      "Epoch: 0, batch: 188 Loss: 0.1305\n",
      "Epoch: 0, batch: 189 Loss: 0.4589\n",
      "Epoch: 0, batch: 190 Loss: 0.4589\n",
      "Epoch: 0, batch: 191 Loss: 0.4589\n",
      "Epoch: 0, batch: 192 Loss: 0.4589\n",
      "Epoch: 0, batch: 193 Loss: 0.4589\n",
      "Epoch: 0, batch: 194 Loss: 0.1307\n",
      "Epoch: 0, batch: 195 Loss: 0.4588\n",
      "Epoch: 0, batch: 196 Loss: 0.1307\n",
      "Epoch: 0, batch: 197 Loss: 0.1306\n",
      "Epoch: 0, batch: 198 Loss: 0.4589\n",
      "Epoch: 0, batch: 199 Loss: 0.7875\n",
      "Epoch: 0, batch: 200 Loss: 0.1304\n",
      "Epoch: 0, batch: 201 Loss: 0.4589\n",
      "Epoch: 0, batch: 202 Loss: 0.7874\n",
      "Epoch: 0, batch: 203 Loss: 0.4589\n",
      "Epoch: 0, batch: 204 Loss: 0.4588\n",
      "Epoch: 0, batch: 205 Loss: 0.1310\n",
      "Epoch: 0, batch: 206 Loss: 0.1311\n",
      "Epoch: 0, batch: 207 Loss: 0.4587\n",
      "Epoch: 0, batch: 208 Loss: 0.4587\n",
      "Epoch: 0, batch: 209 Loss: 0.4587\n",
      "Epoch: 0, batch: 210 Loss: 1.1136\n",
      "Epoch: 0, batch: 211 Loss: 1.1125\n",
      "Epoch: 0, batch: 212 Loss: 0.4583\n",
      "Epoch: 0, batch: 213 Loss: 0.1328\n",
      "Epoch: 0, batch: 214 Loss: 0.4580\n",
      "Epoch: 0, batch: 215 Loss: 0.7820\n",
      "Epoch: 0, batch: 216 Loss: 0.1342\n",
      "Epoch: 0, batch: 217 Loss: 0.4575\n",
      "Epoch: 0, batch: 218 Loss: 0.1349\n",
      "Epoch: 0, batch: 219 Loss: 0.7794\n",
      "Epoch: 0, batch: 220 Loss: 0.1356\n",
      "Epoch: 0, batch: 221 Loss: 0.4571\n",
      "Epoch: 0, batch: 222 Loss: 0.4570\n",
      "Epoch: 0, batch: 223 Loss: 0.1364\n",
      "Epoch: 0, batch: 224 Loss: 0.1365\n",
      "Epoch: 0, batch: 225 Loss: 0.1365\n",
      "Epoch: 0, batch: 226 Loss: 0.1364\n",
      "Epoch: 0, batch: 227 Loss: 0.1363\n",
      "Epoch: 0, batch: 228 Loss: 0.4571\n",
      "Epoch: 0, batch: 229 Loss: 0.7784\n",
      "Epoch: 0, batch: 230 Loss: 0.4571\n",
      "Epoch: 0, batch: 231 Loss: 0.1358\n",
      "Epoch: 0, batch: 232 Loss: 0.1357\n",
      "Epoch: 0, batch: 233 Loss: 0.1356\n",
      "Epoch: 0, batch: 234 Loss: 0.1353\n",
      "Epoch: 0, batch: 235 Loss: 0.4574\n",
      "Epoch: 0, batch: 236 Loss: 0.1347\n",
      "Epoch: 0, batch: 237 Loss: 0.7808\n",
      "Epoch: 0, batch: 238 Loss: 0.4576\n",
      "Epoch: 0, batch: 239 Loss: 0.1341\n",
      "Epoch: 0, batch: 240 Loss: 0.4577\n",
      "Epoch: 0, batch: 241 Loss: 0.1338\n",
      "Epoch: 0, batch: 242 Loss: 0.4578\n",
      "Epoch: 0, batch: 243 Loss: 0.1334\n",
      "Epoch: 0, batch: 244 Loss: 0.1332\n",
      "Epoch: 0, batch: 245 Loss: 0.1329\n",
      "Epoch: 0, batch: 246 Loss: 0.4582\n",
      "Epoch: 0, batch: 247 Loss: 0.1322\n",
      "Epoch: 0, batch: 248 Loss: 0.7850\n",
      "Epoch: 0, batch: 249 Loss: 0.7853\n",
      "Epoch: 0, batch: 250 Loss: 0.1317\n",
      "Epoch: 0, batch: 251 Loss: 0.1316\n",
      "Epoch: 0, batch: 252 Loss: 0.1314\n",
      "Epoch: 0, batch: 253 Loss: 0.1312\n",
      "Epoch: 0, batch: 254 Loss: 0.1308\n",
      "Epoch: 0, batch: 255 Loss: 0.1304\n",
      "Epoch: 0, batch: 256 Loss: 0.4591\n",
      "Epoch: 0, batch: 257 Loss: 0.4592\n",
      "Epoch: 0, batch: 258 Loss: 0.4593\n",
      "Epoch: 0, batch: 259 Loss: 0.4594\n",
      "Epoch: 0, batch: 260 Loss: 1.1205\n",
      "Epoch: 0, batch: 261 Loss: 0.4594\n",
      "Epoch: 0, batch: 262 Loss: 0.1293\n",
      "Epoch: 0, batch: 263 Loss: 0.4593\n",
      "Epoch: 0, batch: 264 Loss: 0.1294\n",
      "Epoch: 0, batch: 265 Loss: 0.4593\n",
      "Epoch: 0, batch: 266 Loss: 0.1294\n",
      "Epoch: 1, batch: 0 Loss: 0.1293\n",
      "Epoch: 1, batch: 1 Loss: 0.1292\n",
      "Epoch: 1, batch: 2 Loss: 0.7900\n",
      "Epoch: 1, batch: 3 Loss: 0.4595\n",
      "Epoch: 1, batch: 4 Loss: 0.1289\n",
      "Epoch: 1, batch: 5 Loss: 0.1288\n",
      "Epoch: 1, batch: 6 Loss: 0.1286\n",
      "Epoch: 1, batch: 7 Loss: 0.4597\n",
      "Epoch: 1, batch: 8 Loss: 0.4598\n",
      "Epoch: 1, batch: 9 Loss: 0.1280\n",
      "Epoch: 1, batch: 10 Loss: 0.4599\n",
      "Epoch: 1, batch: 11 Loss: 0.1276\n",
      "Epoch: 1, batch: 12 Loss: 0.1274\n",
      "Epoch: 1, batch: 13 Loss: 0.7934\n",
      "Epoch: 1, batch: 14 Loss: 0.1269\n",
      "Epoch: 1, batch: 15 Loss: 0.4603\n",
      "Epoch: 1, batch: 16 Loss: 0.1266\n",
      "Epoch: 1, batch: 17 Loss: 0.1264\n",
      "Epoch: 1, batch: 18 Loss: 0.4606\n",
      "Epoch: 1, batch: 19 Loss: 0.7954\n",
      "Epoch: 1, batch: 20 Loss: 0.1259\n",
      "Epoch: 1, batch: 21 Loss: 0.4607\n",
      "Epoch: 1, batch: 22 Loss: 0.1257\n",
      "Epoch: 1, batch: 23 Loss: 0.7961\n",
      "Epoch: 1, batch: 24 Loss: 0.4608\n",
      "Epoch: 1, batch: 25 Loss: 0.4608\n",
      "Epoch: 1, batch: 26 Loss: 0.4608\n",
      "Epoch: 1, batch: 27 Loss: 0.4607\n",
      "Epoch: 1, batch: 28 Loss: 0.4606\n",
      "Epoch: 1, batch: 29 Loss: 0.4606\n",
      "Epoch: 1, batch: 30 Loss: 0.1263\n",
      "Epoch: 1, batch: 31 Loss: 0.4605\n",
      "Epoch: 1, batch: 32 Loss: 0.1265\n",
      "Epoch: 1, batch: 33 Loss: 0.7943\n",
      "Epoch: 1, batch: 34 Loss: 0.1267\n",
      "Epoch: 1, batch: 35 Loss: 0.4603\n",
      "Epoch: 1, batch: 36 Loss: 0.4603\n",
      "Epoch: 1, batch: 37 Loss: 0.1270\n",
      "Epoch: 1, batch: 38 Loss: 0.4602\n",
      "Epoch: 1, batch: 39 Loss: 0.1270\n",
      "Epoch: 1, batch: 40 Loss: 0.1270\n",
      "Epoch: 1, batch: 41 Loss: 0.1269\n",
      "Epoch: 1, batch: 42 Loss: 0.1266\n",
      "Epoch: 1, batch: 43 Loss: 0.7946\n",
      "Epoch: 1, batch: 44 Loss: 0.1263\n",
      "Epoch: 1, batch: 45 Loss: 0.7951\n",
      "Epoch: 1, batch: 46 Loss: 0.4606\n",
      "Epoch: 1, batch: 47 Loss: 0.1262\n",
      "Epoch: 1, batch: 48 Loss: 0.1261\n",
      "Epoch: 1, batch: 49 Loss: 0.1260\n",
      "Epoch: 1, batch: 50 Loss: 0.4607\n",
      "Epoch: 1, batch: 51 Loss: 0.1256\n",
      "Epoch: 1, batch: 52 Loss: 0.1254\n",
      "Epoch: 1, batch: 53 Loss: 0.4610\n",
      "Epoch: 1, batch: 54 Loss: 0.4611\n",
      "Epoch: 1, batch: 55 Loss: 0.7976\n",
      "Epoch: 1, batch: 56 Loss: 0.7976\n",
      "Epoch: 1, batch: 57 Loss: 0.1249\n",
      "Epoch: 1, batch: 58 Loss: 0.7971\n",
      "Epoch: 1, batch: 59 Loss: 0.4610\n",
      "Epoch: 1, batch: 60 Loss: 0.1254\n",
      "Epoch: 1, batch: 61 Loss: 0.4608\n",
      "Epoch: 1, batch: 62 Loss: 0.7958\n",
      "Epoch: 1, batch: 63 Loss: 0.1260\n",
      "Epoch: 1, batch: 64 Loss: 0.1262\n",
      "Epoch: 1, batch: 65 Loss: 0.4605\n",
      "Epoch: 1, batch: 66 Loss: 0.4605\n",
      "Epoch: 1, batch: 67 Loss: 0.1265\n",
      "Epoch: 1, batch: 68 Loss: 0.4604\n",
      "Epoch: 1, batch: 69 Loss: 0.4604\n",
      "Epoch: 1, batch: 70 Loss: 0.4604\n",
      "Epoch: 1, batch: 71 Loss: 0.4603\n",
      "Epoch: 1, batch: 72 Loss: 0.4603\n",
      "Epoch: 1, batch: 73 Loss: 0.1271\n",
      "Epoch: 1, batch: 74 Loss: 0.7932\n",
      "Epoch: 1, batch: 75 Loss: 0.1274\n",
      "Epoch: 1, batch: 76 Loss: 0.4600\n",
      "Epoch: 1, batch: 77 Loss: 0.4600\n",
      "Epoch: 1, batch: 78 Loss: 0.4599\n",
      "Epoch: 1, batch: 79 Loss: 0.1279\n",
      "Epoch: 1, batch: 80 Loss: 0.1280\n",
      "Epoch: 1, batch: 81 Loss: 0.1280\n",
      "Epoch: 1, batch: 82 Loss: 0.1278\n",
      "Epoch: 1, batch: 83 Loss: 0.1276\n",
      "Epoch: 1, batch: 84 Loss: 0.7928\n",
      "Epoch: 1, batch: 85 Loss: 0.1273\n",
      "Epoch: 1, batch: 86 Loss: 0.1271\n",
      "Epoch: 1, batch: 87 Loss: 0.4603\n",
      "Epoch: 1, batch: 88 Loss: 0.1267\n",
      "Epoch: 1, batch: 89 Loss: 0.4605\n",
      "Epoch: 1, batch: 90 Loss: 0.4605\n",
      "Epoch: 1, batch: 91 Loss: 1.1296\n",
      "Epoch: 1, batch: 92 Loss: 0.4605\n",
      "Epoch: 1, batch: 93 Loss: 0.4604\n",
      "Epoch: 1, batch: 94 Loss: 0.4604\n",
      "Epoch: 1, batch: 95 Loss: 0.1269\n",
      "Epoch: 1, batch: 96 Loss: 0.1270\n",
      "Epoch: 1, batch: 97 Loss: 0.4602\n",
      "Epoch: 1, batch: 98 Loss: 0.1271\n",
      "Epoch: 1, batch: 99 Loss: 0.4602\n",
      "Epoch: 1, batch: 100 Loss: 0.1270\n",
      "Epoch: 1, batch: 101 Loss: 0.7937\n",
      "Epoch: 1, batch: 102 Loss: 0.4602\n",
      "Epoch: 1, batch: 103 Loss: 0.7933\n",
      "Epoch: 1, batch: 104 Loss: 0.4601\n",
      "Epoch: 1, batch: 105 Loss: 0.7924\n",
      "Epoch: 1, batch: 106 Loss: 0.7917\n",
      "Epoch: 1, batch: 107 Loss: 0.1285\n",
      "Epoch: 1, batch: 108 Loss: 0.4595\n",
      "Epoch: 1, batch: 109 Loss: 0.1293\n",
      "Epoch: 1, batch: 110 Loss: 0.1296\n",
      "Epoch: 1, batch: 111 Loss: 0.1297\n",
      "Epoch: 1, batch: 112 Loss: 1.4476\n",
      "Epoch: 1, batch: 113 Loss: 0.4590\n",
      "Epoch: 1, batch: 114 Loss: 0.1306\n",
      "Epoch: 1, batch: 115 Loss: 0.7866\n",
      "Epoch: 1, batch: 116 Loss: 0.4586\n",
      "Epoch: 1, batch: 117 Loss: 0.4585\n",
      "Epoch: 1, batch: 118 Loss: 0.7845\n",
      "Epoch: 1, batch: 119 Loss: 0.7836\n",
      "Epoch: 1, batch: 120 Loss: 0.1334\n",
      "Epoch: 1, batch: 121 Loss: 0.1338\n",
      "Epoch: 1, batch: 122 Loss: 0.4576\n",
      "Epoch: 1, batch: 123 Loss: 0.7805\n",
      "Epoch: 1, batch: 124 Loss: 0.1350\n",
      "Epoch: 1, batch: 125 Loss: 0.7792\n",
      "Epoch: 1, batch: 126 Loss: 0.4571\n",
      "Epoch: 1, batch: 127 Loss: 0.1362\n",
      "Epoch: 1, batch: 128 Loss: 0.1365\n",
      "Epoch: 1, batch: 129 Loss: 0.7770\n",
      "Epoch: 1, batch: 130 Loss: 0.1370\n",
      "Epoch: 1, batch: 131 Loss: 0.4567\n",
      "Epoch: 1, batch: 132 Loss: 0.1374\n",
      "Epoch: 1, batch: 133 Loss: 0.4566\n",
      "Epoch: 1, batch: 134 Loss: 0.4566\n",
      "Epoch: 1, batch: 135 Loss: 0.1377\n",
      "Epoch: 1, batch: 136 Loss: 0.1377\n",
      "Epoch: 1, batch: 137 Loss: 0.7756\n",
      "Epoch: 1, batch: 138 Loss: 0.4566\n",
      "Epoch: 1, batch: 139 Loss: 0.4565\n",
      "Epoch: 1, batch: 140 Loss: 0.4565\n",
      "Epoch: 1, batch: 141 Loss: 0.1380\n",
      "Epoch: 1, batch: 142 Loss: 0.1380\n",
      "Epoch: 1, batch: 143 Loss: 0.1379\n",
      "Epoch: 1, batch: 144 Loss: 0.4565\n",
      "Epoch: 1, batch: 145 Loss: 0.1376\n",
      "Epoch: 1, batch: 146 Loss: 0.1374\n",
      "Epoch: 1, batch: 147 Loss: 0.1371\n",
      "Epoch: 1, batch: 148 Loss: 0.4568\n",
      "Epoch: 1, batch: 149 Loss: 0.4569\n",
      "Epoch: 1, batch: 150 Loss: 0.4570\n",
      "Epoch: 1, batch: 151 Loss: 1.0992\n",
      "Epoch: 1, batch: 152 Loss: 0.1361\n",
      "Epoch: 1, batch: 153 Loss: 0.4570\n",
      "Epoch: 1, batch: 154 Loss: 0.4570\n",
      "Epoch: 1, batch: 155 Loss: 1.4192\n",
      "Epoch: 1, batch: 156 Loss: 0.1367\n",
      "Epoch: 1, batch: 157 Loss: 0.1371\n",
      "Epoch: 1, batch: 158 Loss: 0.4567\n",
      "Epoch: 1, batch: 159 Loss: 0.1375\n",
      "Epoch: 1, batch: 160 Loss: 0.1376\n",
      "Epoch: 1, batch: 161 Loss: 0.4566\n",
      "Epoch: 1, batch: 162 Loss: 0.4566\n",
      "Epoch: 1, batch: 163 Loss: 0.7755\n",
      "Epoch: 1, batch: 164 Loss: 0.1378\n",
      "Epoch: 1, batch: 165 Loss: 0.4565\n",
      "Epoch: 1, batch: 166 Loss: 0.4564\n",
      "Epoch: 1, batch: 167 Loss: 0.1381\n",
      "Epoch: 1, batch: 168 Loss: 0.1381\n",
      "Epoch: 1, batch: 169 Loss: 0.4564\n",
      "Epoch: 1, batch: 170 Loss: 0.4565\n",
      "Epoch: 1, batch: 171 Loss: 0.1380\n",
      "Epoch: 1, batch: 172 Loss: 0.1378\n",
      "Epoch: 1, batch: 173 Loss: 0.1376\n",
      "Epoch: 1, batch: 174 Loss: 0.7760\n",
      "Epoch: 1, batch: 175 Loss: 0.1372\n",
      "Epoch: 1, batch: 176 Loss: 0.1370\n",
      "Epoch: 1, batch: 177 Loss: 0.1368\n",
      "Epoch: 1, batch: 178 Loss: 0.1364\n",
      "Epoch: 1, batch: 179 Loss: 0.4571\n",
      "Epoch: 1, batch: 180 Loss: 0.1357\n",
      "Epoch: 1, batch: 181 Loss: 0.1353\n",
      "Epoch: 1, batch: 182 Loss: 0.1348\n",
      "Epoch: 1, batch: 183 Loss: 0.4576\n",
      "Epoch: 1, batch: 184 Loss: 0.4577\n",
      "Epoch: 1, batch: 185 Loss: 0.1335\n",
      "Epoch: 1, batch: 186 Loss: 0.4580\n",
      "Epoch: 1, batch: 187 Loss: 0.1327\n",
      "Epoch: 1, batch: 188 Loss: 0.4583\n",
      "Epoch: 1, batch: 189 Loss: 0.1320\n",
      "Epoch: 1, batch: 190 Loss: 0.4585\n",
      "Epoch: 1, batch: 191 Loss: 0.1313\n",
      "Epoch: 1, batch: 192 Loss: 0.1309\n",
      "Epoch: 1, batch: 193 Loss: 0.4589\n",
      "Epoch: 1, batch: 194 Loss: 0.4590\n",
      "Epoch: 1, batch: 195 Loss: 0.4591\n",
      "Epoch: 1, batch: 196 Loss: 0.7889\n",
      "Epoch: 1, batch: 197 Loss: 0.4592\n",
      "Epoch: 1, batch: 198 Loss: 0.1296\n",
      "Epoch: 1, batch: 199 Loss: 0.4593\n",
      "Epoch: 1, batch: 200 Loss: 0.1294\n",
      "Epoch: 1, batch: 201 Loss: 0.4593\n",
      "Epoch: 1, batch: 202 Loss: 0.1292\n",
      "Epoch: 1, batch: 203 Loss: 0.4594\n",
      "Epoch: 1, batch: 204 Loss: 0.1289\n",
      "Epoch: 1, batch: 205 Loss: 0.7904\n",
      "Epoch: 1, batch: 206 Loss: 0.1287\n",
      "Epoch: 1, batch: 207 Loss: 0.4596\n",
      "Epoch: 1, batch: 208 Loss: 0.4596\n",
      "Epoch: 1, batch: 209 Loss: 0.4597\n",
      "Epoch: 1, batch: 210 Loss: 0.4596\n",
      "Epoch: 1, batch: 211 Loss: 0.1286\n",
      "Epoch: 1, batch: 212 Loss: 0.1285\n",
      "Epoch: 1, batch: 213 Loss: 0.1284\n",
      "Epoch: 1, batch: 214 Loss: 0.1282\n",
      "Epoch: 1, batch: 215 Loss: 0.4599\n",
      "Epoch: 1, batch: 216 Loss: 0.1276\n",
      "Epoch: 1, batch: 217 Loss: 0.1274\n",
      "Epoch: 1, batch: 218 Loss: 0.4602\n",
      "Epoch: 1, batch: 219 Loss: 0.7940\n",
      "Epoch: 1, batch: 220 Loss: 0.1266\n",
      "Epoch: 1, batch: 221 Loss: 0.1265\n",
      "Epoch: 1, batch: 222 Loss: 1.4636\n",
      "Epoch: 1, batch: 223 Loss: 0.4605\n",
      "Epoch: 1, batch: 224 Loss: 0.1266\n",
      "Epoch: 1, batch: 225 Loss: 0.1267\n",
      "Epoch: 1, batch: 226 Loss: 0.1267\n",
      "Epoch: 1, batch: 227 Loss: 0.1266\n",
      "Epoch: 1, batch: 228 Loss: 0.4604\n",
      "Epoch: 1, batch: 229 Loss: 0.1264\n",
      "Epoch: 1, batch: 230 Loss: 0.1262\n",
      "Epoch: 1, batch: 231 Loss: 0.1259\n",
      "Epoch: 1, batch: 232 Loss: 0.1256\n",
      "Epoch: 1, batch: 233 Loss: 0.1252\n",
      "Epoch: 1, batch: 234 Loss: 0.1247\n",
      "Epoch: 1, batch: 235 Loss: 0.1243\n",
      "Epoch: 1, batch: 236 Loss: 0.7994\n",
      "Epoch: 1, batch: 237 Loss: 0.4617\n",
      "Epoch: 1, batch: 238 Loss: 0.1232\n",
      "Epoch: 1, batch: 239 Loss: 0.4620\n",
      "Epoch: 1, batch: 240 Loss: 0.1227\n",
      "Epoch: 1, batch: 241 Loss: 0.1224\n",
      "Epoch: 1, batch: 242 Loss: 0.8027\n",
      "Epoch: 1, batch: 243 Loss: 1.1435\n",
      "Epoch: 1, batch: 244 Loss: 0.4623\n",
      "Epoch: 1, batch: 245 Loss: 0.4623\n",
      "Epoch: 1, batch: 246 Loss: 0.4622\n",
      "Epoch: 1, batch: 247 Loss: 0.4621\n",
      "Epoch: 1, batch: 248 Loss: 0.1229\n",
      "Epoch: 1, batch: 249 Loss: 0.8008\n",
      "Epoch: 1, batch: 250 Loss: 0.1233\n",
      "Epoch: 1, batch: 251 Loss: 0.1234\n",
      "Epoch: 1, batch: 252 Loss: 0.4617\n",
      "Epoch: 1, batch: 253 Loss: 0.7997\n",
      "Epoch: 1, batch: 254 Loss: 0.4615\n",
      "Epoch: 1, batch: 255 Loss: 0.1241\n",
      "Epoch: 1, batch: 256 Loss: 0.7985\n",
      "Epoch: 1, batch: 257 Loss: 0.4612\n",
      "Epoch: 1, batch: 258 Loss: 0.7974\n",
      "Epoch: 1, batch: 259 Loss: 0.4609\n",
      "Epoch: 1, batch: 260 Loss: 0.4608\n",
      "Epoch: 1, batch: 261 Loss: 0.7950\n",
      "Epoch: 1, batch: 262 Loss: 0.1267\n",
      "Epoch: 1, batch: 263 Loss: 0.4602\n",
      "Epoch: 1, batch: 264 Loss: 0.1275\n",
      "Epoch: 1, batch: 265 Loss: 0.1278\n",
      "Epoch: 1, batch: 266 Loss: 0.6258\n",
      "Epoch: 2, batch: 0 Loss: 0.1282\n",
      "Epoch: 2, batch: 1 Loss: 0.4597\n",
      "Epoch: 2, batch: 2 Loss: 0.1284\n",
      "Epoch: 2, batch: 3 Loss: 0.1285\n",
      "Epoch: 2, batch: 4 Loss: 0.4597\n",
      "Epoch: 2, batch: 5 Loss: 0.7910\n",
      "Epoch: 2, batch: 6 Loss: 0.4596\n",
      "Epoch: 2, batch: 7 Loss: 0.7904\n",
      "Epoch: 2, batch: 8 Loss: 0.1290\n",
      "Epoch: 2, batch: 9 Loss: 0.1292\n",
      "Epoch: 2, batch: 10 Loss: 0.4593\n",
      "Epoch: 2, batch: 11 Loss: 0.1294\n",
      "Epoch: 2, batch: 12 Loss: 0.7892\n",
      "Epoch: 2, batch: 13 Loss: 0.7889\n",
      "Epoch: 2, batch: 14 Loss: 0.1299\n",
      "Epoch: 2, batch: 15 Loss: 0.1301\n",
      "Epoch: 2, batch: 16 Loss: 0.1301\n",
      "Epoch: 2, batch: 17 Loss: 0.1301\n",
      "Epoch: 2, batch: 18 Loss: 0.4591\n",
      "Epoch: 2, batch: 19 Loss: 0.4591\n",
      "Epoch: 2, batch: 20 Loss: 0.4591\n",
      "Epoch: 2, batch: 21 Loss: 0.1299\n",
      "Epoch: 2, batch: 22 Loss: 0.4592\n",
      "Epoch: 2, batch: 23 Loss: 0.1298\n",
      "Epoch: 2, batch: 24 Loss: 0.7888\n",
      "Epoch: 2, batch: 25 Loss: 0.1297\n",
      "Epoch: 2, batch: 26 Loss: 0.4592\n",
      "Epoch: 2, batch: 27 Loss: 0.7889\n",
      "Epoch: 2, batch: 28 Loss: 0.1298\n",
      "Epoch: 2, batch: 29 Loss: 0.1298\n",
      "Epoch: 2, batch: 30 Loss: 0.4592\n",
      "Epoch: 2, batch: 31 Loss: 0.1297\n",
      "Epoch: 2, batch: 32 Loss: 0.4592\n",
      "Epoch: 2, batch: 33 Loss: 0.4593\n",
      "Epoch: 2, batch: 34 Loss: 0.1295\n",
      "Epoch: 2, batch: 35 Loss: 0.4593\n",
      "Epoch: 2, batch: 36 Loss: 0.4593\n",
      "Epoch: 2, batch: 37 Loss: 1.1194\n",
      "Epoch: 2, batch: 38 Loss: 0.1296\n",
      "Epoch: 2, batch: 39 Loss: 0.1297\n",
      "Epoch: 2, batch: 40 Loss: 0.1298\n",
      "Epoch: 2, batch: 41 Loss: 0.1297\n",
      "Epoch: 2, batch: 42 Loss: 0.1296\n",
      "Epoch: 2, batch: 43 Loss: 0.4593\n",
      "Epoch: 2, batch: 44 Loss: 0.1292\n",
      "Epoch: 2, batch: 45 Loss: 0.1289\n",
      "Epoch: 2, batch: 46 Loss: 0.1286\n",
      "Epoch: 2, batch: 47 Loss: 0.7913\n",
      "Epoch: 2, batch: 48 Loss: 0.1281\n",
      "Epoch: 2, batch: 49 Loss: 0.7920\n",
      "Epoch: 2, batch: 50 Loss: 0.1278\n",
      "Epoch: 2, batch: 51 Loss: 0.1276\n",
      "Epoch: 2, batch: 52 Loss: 0.1274\n",
      "Epoch: 2, batch: 53 Loss: 0.1271\n",
      "Epoch: 2, batch: 54 Loss: 0.1267\n",
      "Epoch: 2, batch: 55 Loss: 0.1263\n",
      "Epoch: 2, batch: 56 Loss: 0.4607\n",
      "Epoch: 2, batch: 57 Loss: 0.4608\n",
      "Epoch: 2, batch: 58 Loss: 0.1252\n",
      "Epoch: 2, batch: 59 Loss: 1.1337\n",
      "Epoch: 2, batch: 60 Loss: 0.7975\n",
      "Epoch: 2, batch: 61 Loss: 0.4611\n",
      "Epoch: 2, batch: 62 Loss: 0.1251\n",
      "Epoch: 2, batch: 63 Loss: 0.1252\n",
      "Epoch: 2, batch: 64 Loss: 0.1251\n",
      "Epoch: 2, batch: 65 Loss: 0.1250\n",
      "Epoch: 2, batch: 66 Loss: 0.1248\n",
      "Epoch: 2, batch: 67 Loss: 0.7980\n",
      "Epoch: 2, batch: 68 Loss: 0.1244\n",
      "Epoch: 2, batch: 69 Loss: 0.4614\n",
      "Epoch: 2, batch: 70 Loss: 0.4614\n",
      "Epoch: 2, batch: 71 Loss: 0.1241\n",
      "Epoch: 2, batch: 72 Loss: 0.4615\n",
      "Epoch: 2, batch: 73 Loss: 1.8121\n",
      "Epoch: 2, batch: 74 Loss: 0.7983\n",
      "Epoch: 2, batch: 75 Loss: 0.1249\n",
      "Epoch: 2, batch: 76 Loss: 0.4609\n",
      "Epoch: 2, batch: 77 Loss: 0.1258\n",
      "Epoch: 2, batch: 78 Loss: 0.4606\n",
      "Epoch: 2, batch: 79 Loss: 0.7946\n",
      "Epoch: 2, batch: 80 Loss: 0.1268\n",
      "Epoch: 2, batch: 81 Loss: 0.4602\n",
      "Epoch: 2, batch: 82 Loss: 0.4601\n",
      "Epoch: 2, batch: 83 Loss: 0.4599\n",
      "Epoch: 2, batch: 84 Loss: 0.1281\n",
      "Epoch: 2, batch: 85 Loss: 0.4597\n",
      "Epoch: 2, batch: 86 Loss: 0.4597\n",
      "Epoch: 2, batch: 87 Loss: 0.4596\n",
      "Epoch: 2, batch: 88 Loss: 0.4595\n",
      "Epoch: 2, batch: 89 Loss: 0.1292\n",
      "Epoch: 2, batch: 90 Loss: 0.4593\n",
      "Epoch: 2, batch: 91 Loss: 1.1188\n",
      "Epoch: 2, batch: 92 Loss: 0.1299\n",
      "Epoch: 2, batch: 93 Loss: 0.1302\n",
      "Epoch: 2, batch: 94 Loss: 0.1304\n",
      "Epoch: 2, batch: 95 Loss: 0.7874\n",
      "Epoch: 2, batch: 96 Loss: 0.7870\n",
      "Epoch: 2, batch: 97 Loss: 0.4587\n",
      "Epoch: 2, batch: 98 Loss: 0.1314\n",
      "Epoch: 2, batch: 99 Loss: 0.1316\n",
      "Epoch: 2, batch: 100 Loss: 0.1317\n",
      "Epoch: 2, batch: 101 Loss: 0.1317\n",
      "Epoch: 2, batch: 102 Loss: 0.1316\n",
      "Epoch: 2, batch: 103 Loss: 0.4586\n",
      "Epoch: 2, batch: 104 Loss: 0.1313\n",
      "Epoch: 2, batch: 105 Loss: 0.4587\n",
      "Epoch: 2, batch: 106 Loss: 0.1310\n",
      "Epoch: 2, batch: 107 Loss: 0.7869\n",
      "Epoch: 2, batch: 108 Loss: 0.1307\n",
      "Epoch: 2, batch: 109 Loss: 0.1305\n",
      "Epoch: 2, batch: 110 Loss: 0.4590\n",
      "Epoch: 2, batch: 111 Loss: 0.4590\n",
      "Epoch: 2, batch: 112 Loss: 0.7881\n",
      "Epoch: 2, batch: 113 Loss: 0.1301\n",
      "Epoch: 2, batch: 114 Loss: 0.1301\n",
      "Epoch: 2, batch: 115 Loss: 0.7883\n",
      "Epoch: 2, batch: 116 Loss: 0.1300\n",
      "Epoch: 2, batch: 117 Loss: 0.4591\n",
      "Epoch: 2, batch: 118 Loss: 0.1299\n",
      "Epoch: 2, batch: 119 Loss: 0.4591\n",
      "Epoch: 2, batch: 120 Loss: 0.1298\n",
      "Epoch: 2, batch: 121 Loss: 0.1296\n",
      "Epoch: 2, batch: 122 Loss: 0.1294\n",
      "Epoch: 2, batch: 123 Loss: 0.1291\n",
      "Epoch: 2, batch: 124 Loss: 0.1287\n",
      "Epoch: 2, batch: 125 Loss: 0.4597\n",
      "Epoch: 2, batch: 126 Loss: 0.4598\n",
      "Epoch: 2, batch: 127 Loss: 0.1277\n",
      "Epoch: 2, batch: 128 Loss: 0.4601\n",
      "Epoch: 2, batch: 129 Loss: 0.4602\n",
      "Epoch: 2, batch: 130 Loss: 0.1269\n",
      "Epoch: 2, batch: 131 Loss: 0.4604\n",
      "Epoch: 2, batch: 132 Loss: 0.4605\n",
      "Epoch: 2, batch: 133 Loss: 0.1263\n",
      "Epoch: 2, batch: 134 Loss: 0.4606\n",
      "Epoch: 2, batch: 135 Loss: 0.1259\n",
      "Epoch: 2, batch: 136 Loss: 0.4608\n",
      "Epoch: 2, batch: 137 Loss: 1.1316\n",
      "Epoch: 2, batch: 138 Loss: 0.7960\n",
      "Epoch: 2, batch: 139 Loss: 0.1259\n",
      "Epoch: 2, batch: 140 Loss: 0.4606\n",
      "Epoch: 2, batch: 141 Loss: 0.1262\n",
      "Epoch: 2, batch: 142 Loss: 0.4605\n",
      "Epoch: 2, batch: 143 Loss: 0.7945\n",
      "Epoch: 2, batch: 144 Loss: 0.1267\n",
      "Epoch: 2, batch: 145 Loss: 0.7938\n",
      "Epoch: 2, batch: 146 Loss: 0.1271\n",
      "Epoch: 2, batch: 147 Loss: 0.4601\n",
      "Epoch: 2, batch: 148 Loss: 0.1275\n",
      "Epoch: 2, batch: 149 Loss: 0.4600\n",
      "Epoch: 2, batch: 150 Loss: 0.4600\n",
      "Epoch: 2, batch: 151 Loss: 0.7920\n",
      "Epoch: 2, batch: 152 Loss: 0.4598\n",
      "Epoch: 2, batch: 153 Loss: 0.1284\n",
      "Epoch: 2, batch: 154 Loss: 0.1285\n",
      "Epoch: 2, batch: 155 Loss: 0.1286\n",
      "Epoch: 2, batch: 156 Loss: 0.7907\n",
      "Epoch: 2, batch: 157 Loss: 0.1287\n",
      "Epoch: 2, batch: 158 Loss: 0.7904\n",
      "Epoch: 2, batch: 159 Loss: 0.4595\n",
      "Epoch: 2, batch: 160 Loss: 0.1291\n",
      "Epoch: 2, batch: 161 Loss: 0.4594\n",
      "Epoch: 2, batch: 162 Loss: 0.1293\n",
      "Epoch: 2, batch: 163 Loss: 0.1293\n",
      "Epoch: 2, batch: 164 Loss: 0.4594\n",
      "Epoch: 2, batch: 165 Loss: 0.4594\n",
      "Epoch: 2, batch: 166 Loss: 0.4594\n",
      "Epoch: 2, batch: 167 Loss: 0.1292\n",
      "Epoch: 2, batch: 168 Loss: 0.1292\n",
      "Epoch: 2, batch: 169 Loss: 0.4595\n",
      "Epoch: 2, batch: 170 Loss: 0.7901\n",
      "Epoch: 2, batch: 171 Loss: 0.4595\n",
      "Epoch: 2, batch: 172 Loss: 0.4594\n",
      "Epoch: 2, batch: 173 Loss: 0.1292\n",
      "Epoch: 2, batch: 174 Loss: 0.4594\n",
      "Epoch: 2, batch: 175 Loss: 0.4594\n",
      "Epoch: 2, batch: 176 Loss: 0.1294\n",
      "Epoch: 2, batch: 177 Loss: 0.1293\n",
      "Epoch: 2, batch: 178 Loss: 0.1292\n",
      "Epoch: 2, batch: 179 Loss: 0.1290\n",
      "Epoch: 2, batch: 180 Loss: 0.4595\n",
      "Epoch: 2, batch: 181 Loss: 0.4596\n",
      "Epoch: 2, batch: 182 Loss: 0.4597\n",
      "Epoch: 2, batch: 183 Loss: 1.1225\n",
      "Epoch: 2, batch: 184 Loss: 0.1285\n",
      "Epoch: 2, batch: 185 Loss: 0.1286\n",
      "Epoch: 2, batch: 186 Loss: 0.4596\n",
      "Epoch: 2, batch: 187 Loss: 1.1215\n",
      "Epoch: 2, batch: 188 Loss: 0.4595\n",
      "Epoch: 2, batch: 189 Loss: 0.4594\n",
      "Epoch: 2, batch: 190 Loss: 0.4592\n",
      "Epoch: 2, batch: 191 Loss: 0.1299\n",
      "Epoch: 2, batch: 192 Loss: 0.1301\n",
      "Epoch: 2, batch: 193 Loss: 0.4590\n",
      "Epoch: 2, batch: 194 Loss: 0.1303\n",
      "Epoch: 2, batch: 195 Loss: 0.4590\n",
      "Epoch: 2, batch: 196 Loss: 0.1304\n",
      "Epoch: 2, batch: 197 Loss: 0.1303\n",
      "Epoch: 2, batch: 198 Loss: 0.4590\n",
      "Epoch: 2, batch: 199 Loss: 0.4591\n",
      "Epoch: 2, batch: 200 Loss: 0.4591\n",
      "Epoch: 2, batch: 201 Loss: 0.4591\n",
      "Epoch: 2, batch: 202 Loss: 0.4591\n",
      "Epoch: 2, batch: 203 Loss: 0.7881\n",
      "Epoch: 2, batch: 204 Loss: 0.4590\n",
      "Epoch: 2, batch: 205 Loss: 1.4441\n",
      "Epoch: 2, batch: 206 Loss: 0.7862\n",
      "Epoch: 2, batch: 207 Loss: 1.1115\n",
      "Epoch: 2, batch: 208 Loss: 0.1328\n",
      "Epoch: 2, batch: 209 Loss: 0.4578\n",
      "Epoch: 2, batch: 210 Loss: 0.1343\n",
      "Epoch: 2, batch: 211 Loss: 0.1348\n",
      "Epoch: 2, batch: 212 Loss: 0.1352\n",
      "Epoch: 2, batch: 213 Loss: 0.4572\n",
      "Epoch: 2, batch: 214 Loss: 0.1358\n",
      "Epoch: 2, batch: 215 Loss: 0.1359\n",
      "Epoch: 2, batch: 216 Loss: 0.7782\n",
      "Epoch: 2, batch: 217 Loss: 0.4570\n",
      "Epoch: 2, batch: 218 Loss: 0.4570\n",
      "Epoch: 2, batch: 219 Loss: 0.4569\n",
      "Epoch: 2, batch: 220 Loss: 0.1368\n",
      "Epoch: 2, batch: 221 Loss: 0.1369\n",
      "Epoch: 2, batch: 222 Loss: 0.1368\n",
      "Epoch: 2, batch: 223 Loss: 0.4568\n",
      "Epoch: 2, batch: 224 Loss: 0.1367\n",
      "Epoch: 2, batch: 225 Loss: 0.4569\n",
      "Epoch: 2, batch: 226 Loss: 0.7775\n",
      "Epoch: 2, batch: 227 Loss: 0.1364\n",
      "Epoch: 2, batch: 228 Loss: 0.7775\n",
      "Epoch: 2, batch: 229 Loss: 0.1365\n",
      "Epoch: 2, batch: 230 Loss: 0.4569\n",
      "Epoch: 2, batch: 231 Loss: 0.4569\n",
      "Epoch: 2, batch: 232 Loss: 0.7771\n",
      "Epoch: 2, batch: 233 Loss: 0.1368\n",
      "Epoch: 2, batch: 234 Loss: 0.1369\n",
      "Epoch: 2, batch: 235 Loss: 1.0965\n",
      "Epoch: 2, batch: 236 Loss: 0.4567\n",
      "Epoch: 2, batch: 237 Loss: 0.1375\n",
      "Epoch: 2, batch: 238 Loss: 0.1376\n",
      "Epoch: 2, batch: 239 Loss: 0.1377\n",
      "Epoch: 2, batch: 240 Loss: 0.4566\n",
      "Epoch: 2, batch: 241 Loss: 0.1376\n",
      "Epoch: 2, batch: 242 Loss: 0.1374\n",
      "Epoch: 2, batch: 243 Loss: 0.7762\n",
      "Epoch: 2, batch: 244 Loss: 0.7762\n",
      "Epoch: 2, batch: 245 Loss: 0.1373\n",
      "Epoch: 2, batch: 246 Loss: 0.1373\n",
      "Epoch: 2, batch: 247 Loss: 0.1372\n",
      "Epoch: 2, batch: 248 Loss: 0.1371\n",
      "Epoch: 2, batch: 249 Loss: 0.4568\n",
      "Epoch: 2, batch: 250 Loss: 0.1366\n",
      "Epoch: 2, batch: 251 Loss: 0.1363\n",
      "Epoch: 2, batch: 252 Loss: 0.4571\n",
      "Epoch: 2, batch: 253 Loss: 0.4572\n",
      "Epoch: 2, batch: 254 Loss: 0.4572\n",
      "Epoch: 2, batch: 255 Loss: 0.1353\n",
      "Epoch: 2, batch: 256 Loss: 0.1350\n",
      "Epoch: 2, batch: 257 Loss: 0.1347\n",
      "Epoch: 2, batch: 258 Loss: 0.1343\n",
      "Epoch: 2, batch: 259 Loss: 0.4578\n",
      "Epoch: 2, batch: 260 Loss: 0.4579\n",
      "Epoch: 2, batch: 261 Loss: 0.4580\n",
      "Epoch: 2, batch: 262 Loss: 0.4581\n",
      "Epoch: 2, batch: 263 Loss: 0.1327\n",
      "Epoch: 2, batch: 264 Loss: 0.4582\n",
      "Epoch: 2, batch: 265 Loss: 0.1323\n",
      "Epoch: 2, batch: 266 Loss: 0.6216\n",
      "Epoch: 3, batch: 0 Loss: 0.1318\n",
      "Epoch: 3, batch: 1 Loss: 0.4585\n",
      "Epoch: 3, batch: 2 Loss: 0.1314\n",
      "Epoch: 3, batch: 3 Loss: 1.4412\n",
      "Epoch: 3, batch: 4 Loss: 0.7858\n",
      "Epoch: 3, batch: 5 Loss: 0.4585\n",
      "Epoch: 3, batch: 6 Loss: 0.1320\n",
      "Epoch: 3, batch: 7 Loss: 0.1322\n",
      "Epoch: 3, batch: 8 Loss: 0.4583\n",
      "Epoch: 3, batch: 9 Loss: 0.1324\n",
      "Epoch: 3, batch: 10 Loss: 0.4582\n",
      "Epoch: 3, batch: 11 Loss: 0.4582\n",
      "Epoch: 3, batch: 12 Loss: 0.4582\n",
      "Epoch: 3, batch: 13 Loss: 0.1327\n",
      "Epoch: 3, batch: 14 Loss: 0.4581\n",
      "Epoch: 3, batch: 15 Loss: 0.1327\n",
      "Epoch: 3, batch: 16 Loss: 0.1326\n",
      "Epoch: 3, batch: 17 Loss: 0.7840\n",
      "Epoch: 3, batch: 18 Loss: 0.1324\n",
      "Epoch: 3, batch: 19 Loss: 0.7841\n",
      "Epoch: 3, batch: 20 Loss: 0.1325\n",
      "Epoch: 3, batch: 21 Loss: 0.1324\n",
      "Epoch: 3, batch: 22 Loss: 0.4583\n",
      "Epoch: 3, batch: 23 Loss: 0.1323\n",
      "Epoch: 3, batch: 24 Loss: 0.1321\n",
      "Epoch: 3, batch: 25 Loss: 0.1319\n",
      "Epoch: 3, batch: 26 Loss: 1.1125\n",
      "Epoch: 3, batch: 27 Loss: 0.4585\n",
      "Epoch: 3, batch: 28 Loss: 0.1316\n",
      "Epoch: 3, batch: 29 Loss: 0.1316\n",
      "Epoch: 3, batch: 30 Loss: 0.1314\n",
      "Epoch: 3, batch: 31 Loss: 0.4587\n",
      "Epoch: 3, batch: 32 Loss: 0.4587\n",
      "Epoch: 3, batch: 33 Loss: 0.1309\n",
      "Epoch: 3, batch: 34 Loss: 0.1307\n",
      "Epoch: 3, batch: 35 Loss: 0.1304\n",
      "Epoch: 3, batch: 36 Loss: 0.7880\n",
      "Epoch: 3, batch: 37 Loss: 0.1299\n",
      "Epoch: 3, batch: 38 Loss: 0.1297\n",
      "Epoch: 3, batch: 39 Loss: 0.1294\n",
      "Epoch: 3, batch: 40 Loss: 0.4594\n",
      "Epoch: 3, batch: 41 Loss: 1.1211\n",
      "Epoch: 3, batch: 42 Loss: 0.1288\n",
      "Epoch: 3, batch: 43 Loss: 0.4596\n",
      "Epoch: 3, batch: 44 Loss: 0.4596\n",
      "Epoch: 3, batch: 45 Loss: 0.4596\n",
      "Epoch: 3, batch: 46 Loss: 0.1288\n",
      "Epoch: 3, batch: 47 Loss: 0.7904\n",
      "Epoch: 3, batch: 48 Loss: 0.4595\n",
      "Epoch: 3, batch: 49 Loss: 1.1204\n",
      "Epoch: 3, batch: 50 Loss: 0.4593\n",
      "Epoch: 3, batch: 51 Loss: 0.7885\n",
      "Epoch: 3, batch: 52 Loss: 0.7875\n",
      "Epoch: 3, batch: 53 Loss: 0.1310\n",
      "Epoch: 3, batch: 54 Loss: 0.1315\n",
      "Epoch: 3, batch: 55 Loss: 0.4584\n",
      "Epoch: 3, batch: 56 Loss: 0.1322\n",
      "Epoch: 3, batch: 57 Loss: 0.1324\n",
      "Epoch: 3, batch: 58 Loss: 0.4582\n",
      "Epoch: 3, batch: 59 Loss: 0.4582\n",
      "Epoch: 3, batch: 60 Loss: 0.1328\n",
      "Epoch: 3, batch: 61 Loss: 0.4581\n",
      "Epoch: 3, batch: 62 Loss: 0.1329\n",
      "Epoch: 3, batch: 63 Loss: 0.7833\n",
      "Epoch: 3, batch: 64 Loss: 0.4580\n",
      "Epoch: 3, batch: 65 Loss: 0.1331\n",
      "Epoch: 3, batch: 66 Loss: 0.4580\n",
      "Epoch: 3, batch: 67 Loss: 0.4580\n",
      "Epoch: 3, batch: 68 Loss: 0.4579\n",
      "Epoch: 3, batch: 69 Loss: 0.1334\n",
      "Epoch: 3, batch: 70 Loss: 0.4579\n",
      "Epoch: 3, batch: 71 Loss: 0.4579\n",
      "Epoch: 3, batch: 72 Loss: 0.4579\n",
      "Epoch: 3, batch: 73 Loss: 0.4578\n",
      "Epoch: 3, batch: 74 Loss: 0.1337\n",
      "Epoch: 3, batch: 75 Loss: 0.4578\n",
      "Epoch: 3, batch: 76 Loss: 0.1338\n",
      "Epoch: 3, batch: 77 Loss: 0.7819\n",
      "Epoch: 3, batch: 78 Loss: 0.1338\n",
      "Epoch: 3, batch: 79 Loss: 0.1338\n",
      "Epoch: 3, batch: 80 Loss: 0.4578\n",
      "Epoch: 3, batch: 81 Loss: 0.1336\n",
      "Epoch: 3, batch: 82 Loss: 0.1335\n",
      "Epoch: 3, batch: 83 Loss: 0.1333\n",
      "Epoch: 3, batch: 84 Loss: 0.1329\n",
      "Epoch: 3, batch: 85 Loss: 0.4582\n",
      "Epoch: 3, batch: 86 Loss: 0.4583\n",
      "Epoch: 3, batch: 87 Loss: 0.4584\n",
      "Epoch: 3, batch: 88 Loss: 0.1318\n",
      "Epoch: 3, batch: 89 Loss: 0.1316\n",
      "Epoch: 3, batch: 90 Loss: 0.1313\n",
      "Epoch: 3, batch: 91 Loss: 0.7867\n",
      "Epoch: 3, batch: 92 Loss: 0.4588\n",
      "Epoch: 3, batch: 93 Loss: 0.4589\n",
      "Epoch: 3, batch: 94 Loss: 0.1305\n",
      "Epoch: 3, batch: 95 Loss: 0.1303\n",
      "Epoch: 3, batch: 96 Loss: 0.7881\n",
      "Epoch: 3, batch: 97 Loss: 0.1300\n",
      "Epoch: 3, batch: 98 Loss: 0.1298\n",
      "Epoch: 3, batch: 99 Loss: 0.4592\n",
      "Epoch: 3, batch: 100 Loss: 0.1294\n",
      "Epoch: 3, batch: 101 Loss: 0.7896\n",
      "Epoch: 3, batch: 102 Loss: 0.1291\n",
      "Epoch: 3, batch: 103 Loss: 0.4595\n",
      "Epoch: 3, batch: 104 Loss: 0.4595\n",
      "Epoch: 3, batch: 105 Loss: 0.1288\n",
      "Epoch: 3, batch: 106 Loss: 0.7905\n",
      "Epoch: 3, batch: 107 Loss: 0.1287\n",
      "Epoch: 3, batch: 108 Loss: 0.1287\n",
      "Epoch: 3, batch: 109 Loss: 0.4596\n",
      "Epoch: 3, batch: 110 Loss: 0.1284\n",
      "Epoch: 3, batch: 111 Loss: 0.1282\n",
      "Epoch: 3, batch: 112 Loss: 0.4598\n",
      "Epoch: 3, batch: 113 Loss: 0.1278\n",
      "Epoch: 3, batch: 114 Loss: 0.1275\n",
      "Epoch: 3, batch: 115 Loss: 0.4602\n",
      "Epoch: 3, batch: 116 Loss: 0.1269\n",
      "Epoch: 3, batch: 117 Loss: 0.4604\n",
      "Epoch: 3, batch: 118 Loss: 0.1264\n",
      "Epoch: 3, batch: 119 Loss: 0.4606\n",
      "Epoch: 3, batch: 120 Loss: 0.1258\n",
      "Epoch: 3, batch: 121 Loss: 0.1255\n",
      "Epoch: 3, batch: 122 Loss: 0.4610\n",
      "Epoch: 3, batch: 123 Loss: 0.7975\n",
      "Epoch: 3, batch: 124 Loss: 0.1247\n",
      "Epoch: 3, batch: 125 Loss: 0.4613\n",
      "Epoch: 3, batch: 126 Loss: 0.4613\n",
      "Epoch: 3, batch: 127 Loss: 0.1243\n",
      "Epoch: 3, batch: 128 Loss: 0.1241\n",
      "Epoch: 3, batch: 129 Loss: 0.4615\n",
      "Epoch: 3, batch: 130 Loss: 1.1373\n",
      "Epoch: 3, batch: 131 Loss: 0.4615\n",
      "Epoch: 3, batch: 132 Loss: 0.1240\n",
      "Epoch: 3, batch: 133 Loss: 0.1241\n",
      "Epoch: 3, batch: 134 Loss: 0.4614\n",
      "Epoch: 3, batch: 135 Loss: 0.4614\n",
      "Epoch: 3, batch: 136 Loss: 0.4614\n",
      "Epoch: 3, batch: 137 Loss: 0.1242\n",
      "Epoch: 3, batch: 138 Loss: 0.4614\n",
      "Epoch: 3, batch: 139 Loss: 0.1242\n",
      "Epoch: 3, batch: 140 Loss: 0.1241\n",
      "Epoch: 3, batch: 141 Loss: 0.1240\n",
      "Epoch: 3, batch: 142 Loss: 0.4616\n",
      "Epoch: 3, batch: 143 Loss: 0.4617\n",
      "Epoch: 3, batch: 144 Loss: 0.8000\n",
      "Epoch: 3, batch: 145 Loss: 0.7998\n",
      "Epoch: 3, batch: 146 Loss: 0.4616\n",
      "Epoch: 3, batch: 147 Loss: 0.4615\n",
      "Epoch: 3, batch: 148 Loss: 0.1242\n",
      "Epoch: 3, batch: 149 Loss: 0.1244\n",
      "Epoch: 3, batch: 150 Loss: 0.7982\n",
      "Epoch: 3, batch: 151 Loss: 0.4612\n",
      "Epoch: 3, batch: 152 Loss: 0.1248\n",
      "Epoch: 3, batch: 153 Loss: 0.4611\n",
      "Epoch: 3, batch: 154 Loss: 0.1251\n",
      "Epoch: 3, batch: 155 Loss: 0.4610\n",
      "Epoch: 3, batch: 156 Loss: 0.4610\n",
      "Epoch: 3, batch: 157 Loss: 0.1253\n",
      "Epoch: 3, batch: 158 Loss: 1.1323\n",
      "Epoch: 3, batch: 159 Loss: 0.4608\n",
      "Epoch: 3, batch: 160 Loss: 0.4607\n",
      "Epoch: 3, batch: 161 Loss: 0.1262\n",
      "Epoch: 3, batch: 162 Loss: 0.1264\n",
      "Epoch: 3, batch: 163 Loss: 0.1264\n",
      "Epoch: 3, batch: 164 Loss: 0.1264\n",
      "Epoch: 3, batch: 165 Loss: 0.4605\n",
      "Epoch: 3, batch: 166 Loss: 0.1262\n",
      "Epoch: 3, batch: 167 Loss: 0.7951\n",
      "Epoch: 3, batch: 168 Loss: 0.1261\n",
      "Epoch: 3, batch: 169 Loss: 0.1260\n",
      "Epoch: 3, batch: 170 Loss: 1.1302\n",
      "Epoch: 3, batch: 171 Loss: 0.1261\n",
      "Epoch: 3, batch: 172 Loss: 0.7951\n",
      "Epoch: 3, batch: 173 Loss: 0.1263\n",
      "Epoch: 3, batch: 174 Loss: 0.4605\n",
      "Epoch: 3, batch: 175 Loss: 0.1266\n",
      "Epoch: 3, batch: 176 Loss: 0.1266\n",
      "Epoch: 3, batch: 177 Loss: 0.4604\n",
      "Epoch: 3, batch: 178 Loss: 0.7944\n",
      "Epoch: 3, batch: 179 Loss: 0.4604\n",
      "Epoch: 3, batch: 180 Loss: 0.7938\n",
      "Epoch: 3, batch: 181 Loss: 0.1271\n",
      "Epoch: 3, batch: 182 Loss: 0.1273\n",
      "Epoch: 3, batch: 183 Loss: 0.7928\n",
      "Epoch: 3, batch: 184 Loss: 0.1276\n",
      "Epoch: 3, batch: 185 Loss: 0.4599\n",
      "Epoch: 3, batch: 186 Loss: 0.1279\n",
      "Epoch: 3, batch: 187 Loss: 1.1238\n",
      "Epoch: 3, batch: 188 Loss: 0.4597\n",
      "Epoch: 3, batch: 189 Loss: 0.1286\n",
      "Epoch: 3, batch: 190 Loss: 0.4595\n",
      "Epoch: 3, batch: 191 Loss: 0.4595\n",
      "Epoch: 3, batch: 192 Loss: 0.1292\n",
      "Epoch: 3, batch: 193 Loss: 0.1293\n",
      "Epoch: 3, batch: 194 Loss: 0.7893\n",
      "Epoch: 3, batch: 195 Loss: 0.4593\n",
      "Epoch: 3, batch: 196 Loss: 0.1297\n",
      "Epoch: 3, batch: 197 Loss: 0.4592\n",
      "Epoch: 3, batch: 198 Loss: 0.4591\n",
      "Epoch: 3, batch: 199 Loss: 0.1300\n",
      "Epoch: 3, batch: 200 Loss: 0.4591\n",
      "Epoch: 3, batch: 201 Loss: 0.1301\n",
      "Epoch: 3, batch: 202 Loss: 0.4591\n",
      "Epoch: 3, batch: 203 Loss: 0.4591\n",
      "Epoch: 3, batch: 204 Loss: 1.1170\n",
      "Epoch: 3, batch: 205 Loss: 0.4589\n",
      "Epoch: 3, batch: 206 Loss: 0.4588\n",
      "Epoch: 3, batch: 207 Loss: 0.4587\n",
      "Epoch: 3, batch: 208 Loss: 0.1314\n",
      "Epoch: 3, batch: 209 Loss: 0.1316\n",
      "Epoch: 3, batch: 210 Loss: 0.1317\n",
      "Epoch: 3, batch: 211 Loss: 0.1317\n",
      "Epoch: 3, batch: 212 Loss: 0.4585\n",
      "Epoch: 3, batch: 213 Loss: 0.1315\n",
      "Epoch: 3, batch: 214 Loss: 0.1313\n",
      "Epoch: 3, batch: 215 Loss: 0.4587\n",
      "Epoch: 3, batch: 216 Loss: 0.1309\n",
      "Epoch: 3, batch: 217 Loss: 0.4589\n",
      "Epoch: 3, batch: 218 Loss: 0.4589\n",
      "Epoch: 3, batch: 219 Loss: 0.1303\n",
      "Epoch: 3, batch: 220 Loss: 0.1301\n",
      "Epoch: 3, batch: 221 Loss: 0.4592\n",
      "Epoch: 3, batch: 222 Loss: 0.4593\n",
      "Epoch: 3, batch: 223 Loss: 0.4593\n",
      "Epoch: 3, batch: 224 Loss: 0.1292\n",
      "Epoch: 3, batch: 225 Loss: 0.1290\n",
      "Epoch: 3, batch: 226 Loss: 0.4596\n",
      "Epoch: 3, batch: 227 Loss: 0.4596\n",
      "Epoch: 3, batch: 228 Loss: 0.4597\n",
      "Epoch: 3, batch: 229 Loss: 0.1283\n",
      "Epoch: 3, batch: 230 Loss: 0.1281\n",
      "Epoch: 3, batch: 231 Loss: 0.4599\n",
      "Epoch: 3, batch: 232 Loss: 0.4600\n",
      "Epoch: 3, batch: 233 Loss: 1.1252\n",
      "Epoch: 3, batch: 234 Loss: 0.1276\n",
      "Epoch: 3, batch: 235 Loss: 0.7922\n",
      "Epoch: 3, batch: 236 Loss: 0.1279\n",
      "Epoch: 3, batch: 237 Loss: 0.4598\n",
      "Epoch: 3, batch: 238 Loss: 0.1282\n",
      "Epoch: 3, batch: 239 Loss: 0.1282\n",
      "Epoch: 3, batch: 240 Loss: 0.7915\n",
      "Epoch: 3, batch: 241 Loss: 0.1282\n",
      "Epoch: 3, batch: 242 Loss: 0.1282\n",
      "Epoch: 3, batch: 243 Loss: 0.1281\n",
      "Epoch: 3, batch: 244 Loss: 0.7918\n",
      "Epoch: 3, batch: 245 Loss: 0.1279\n",
      "Epoch: 3, batch: 246 Loss: 0.4599\n",
      "Epoch: 3, batch: 247 Loss: 0.7921\n",
      "Epoch: 3, batch: 248 Loss: 0.4599\n",
      "Epoch: 3, batch: 249 Loss: 0.4598\n",
      "Epoch: 3, batch: 250 Loss: 0.4598\n",
      "Epoch: 3, batch: 251 Loss: 0.1284\n",
      "Epoch: 3, batch: 252 Loss: 0.1285\n",
      "Epoch: 3, batch: 253 Loss: 0.4597\n",
      "Epoch: 3, batch: 254 Loss: 0.7908\n",
      "Epoch: 3, batch: 255 Loss: 0.4596\n",
      "Epoch: 3, batch: 256 Loss: 0.4595\n",
      "Epoch: 3, batch: 257 Loss: 0.4594\n",
      "Epoch: 3, batch: 258 Loss: 0.1293\n",
      "Epoch: 3, batch: 259 Loss: 0.7892\n",
      "Epoch: 3, batch: 260 Loss: 0.1297\n",
      "Epoch: 3, batch: 261 Loss: 0.1298\n",
      "Epoch: 3, batch: 262 Loss: 0.4591\n",
      "Epoch: 3, batch: 263 Loss: 1.1175\n",
      "Epoch: 3, batch: 264 Loss: 0.1303\n",
      "Epoch: 3, batch: 265 Loss: 0.4589\n",
      "Epoch: 3, batch: 266 Loss: 0.1307\n",
      "Epoch: 4, batch: 0 Loss: 0.1309\n",
      "Epoch: 4, batch: 1 Loss: 0.1309\n",
      "Epoch: 4, batch: 2 Loss: 0.1308\n",
      "Epoch: 4, batch: 3 Loss: 0.1306\n",
      "Epoch: 4, batch: 4 Loss: 0.1303\n",
      "Epoch: 4, batch: 5 Loss: 0.1300\n",
      "Epoch: 4, batch: 6 Loss: 0.4592\n",
      "Epoch: 4, batch: 7 Loss: 0.4593\n",
      "Epoch: 4, batch: 8 Loss: 0.1291\n",
      "Epoch: 4, batch: 9 Loss: 0.1288\n",
      "Epoch: 4, batch: 10 Loss: 0.1284\n",
      "Epoch: 4, batch: 11 Loss: 0.4599\n",
      "Epoch: 4, batch: 12 Loss: 0.1276\n",
      "Epoch: 4, batch: 13 Loss: 0.1272\n",
      "Epoch: 4, batch: 14 Loss: 0.4603\n",
      "Epoch: 4, batch: 15 Loss: 0.1264\n",
      "Epoch: 4, batch: 16 Loss: 0.1259\n",
      "Epoch: 4, batch: 17 Loss: 0.4609\n",
      "Epoch: 4, batch: 18 Loss: 0.4610\n",
      "Epoch: 4, batch: 19 Loss: 0.1247\n",
      "Epoch: 4, batch: 20 Loss: 0.4613\n",
      "Epoch: 4, batch: 21 Loss: 0.1241\n",
      "Epoch: 4, batch: 22 Loss: 0.1237\n",
      "Epoch: 4, batch: 23 Loss: 0.8003\n",
      "Epoch: 4, batch: 24 Loss: 0.1231\n",
      "Epoch: 4, batch: 25 Loss: 0.8011\n",
      "Epoch: 4, batch: 26 Loss: 1.1406\n",
      "Epoch: 4, batch: 27 Loss: 0.8009\n",
      "Epoch: 4, batch: 28 Loss: 0.1233\n",
      "Epoch: 4, batch: 29 Loss: 0.4616\n",
      "Epoch: 4, batch: 30 Loss: 0.7992\n",
      "Epoch: 4, batch: 31 Loss: 0.4614\n",
      "Epoch: 4, batch: 32 Loss: 0.1247\n",
      "Epoch: 4, batch: 33 Loss: 0.1249\n",
      "Epoch: 4, batch: 34 Loss: 0.1251\n",
      "Epoch: 4, batch: 35 Loss: 1.1327\n",
      "Epoch: 4, batch: 36 Loss: 0.4609\n",
      "Epoch: 4, batch: 37 Loss: 0.4607\n",
      "Epoch: 4, batch: 38 Loss: 0.4606\n",
      "Epoch: 4, batch: 39 Loss: 0.1265\n",
      "Epoch: 4, batch: 40 Loss: 0.1267\n",
      "Epoch: 4, batch: 41 Loss: 0.1268\n",
      "Epoch: 4, batch: 42 Loss: 0.1268\n",
      "Epoch: 4, batch: 43 Loss: 0.1267\n",
      "Epoch: 4, batch: 44 Loss: 0.1266\n",
      "Epoch: 4, batch: 45 Loss: 0.7947\n",
      "Epoch: 4, batch: 46 Loss: 0.7948\n",
      "Epoch: 4, batch: 47 Loss: 0.4605\n",
      "Epoch: 4, batch: 48 Loss: 0.4604\n",
      "Epoch: 4, batch: 49 Loss: 0.1267\n",
      "Epoch: 4, batch: 50 Loss: 0.7939\n",
      "Epoch: 4, batch: 51 Loss: 0.1270\n",
      "Epoch: 4, batch: 52 Loss: 0.1271\n",
      "Epoch: 4, batch: 53 Loss: 0.1271\n",
      "Epoch: 4, batch: 54 Loss: 0.4602\n",
      "Epoch: 4, batch: 55 Loss: 0.4603\n",
      "Epoch: 4, batch: 56 Loss: 0.4603\n",
      "Epoch: 4, batch: 57 Loss: 0.7935\n",
      "Epoch: 4, batch: 58 Loss: 0.1272\n",
      "Epoch: 4, batch: 59 Loss: 0.1272\n",
      "Epoch: 4, batch: 60 Loss: 0.1272\n",
      "Epoch: 4, batch: 61 Loss: 0.1271\n",
      "Epoch: 4, batch: 62 Loss: 0.1269\n",
      "Epoch: 4, batch: 63 Loss: 0.7941\n",
      "Epoch: 4, batch: 64 Loss: 0.1266\n",
      "Epoch: 4, batch: 65 Loss: 0.4605\n",
      "Epoch: 4, batch: 66 Loss: 0.1263\n",
      "Epoch: 4, batch: 67 Loss: 0.4606\n",
      "Epoch: 4, batch: 68 Loss: 0.1260\n",
      "Epoch: 4, batch: 69 Loss: 0.1258\n",
      "Epoch: 4, batch: 70 Loss: 0.4608\n",
      "Epoch: 4, batch: 71 Loss: 0.7965\n",
      "Epoch: 4, batch: 72 Loss: 0.1253\n",
      "Epoch: 4, batch: 73 Loss: 0.1252\n",
      "Epoch: 4, batch: 74 Loss: 0.1250\n",
      "Epoch: 4, batch: 75 Loss: 0.1247\n",
      "Epoch: 4, batch: 76 Loss: 0.1244\n",
      "Epoch: 4, batch: 77 Loss: 0.4615\n",
      "Epoch: 4, batch: 78 Loss: 0.1237\n",
      "Epoch: 4, batch: 79 Loss: 0.4618\n",
      "Epoch: 4, batch: 80 Loss: 0.1231\n",
      "Epoch: 4, batch: 81 Loss: 0.1227\n",
      "Epoch: 4, batch: 82 Loss: 0.1223\n",
      "Epoch: 4, batch: 83 Loss: 0.1219\n",
      "Epoch: 4, batch: 84 Loss: 0.1214\n",
      "Epoch: 4, batch: 85 Loss: 1.4890\n",
      "Epoch: 4, batch: 86 Loss: 0.1208\n",
      "Epoch: 4, batch: 87 Loss: 0.1207\n",
      "Epoch: 4, batch: 88 Loss: 0.1204\n",
      "Epoch: 4, batch: 89 Loss: 0.8062\n",
      "Epoch: 4, batch: 90 Loss: 0.4632\n",
      "Epoch: 4, batch: 91 Loss: 0.1201\n",
      "Epoch: 4, batch: 92 Loss: 0.4633\n",
      "Epoch: 4, batch: 93 Loss: 0.1199\n",
      "Epoch: 4, batch: 94 Loss: 0.4634\n",
      "Epoch: 4, batch: 95 Loss: 0.8072\n",
      "Epoch: 4, batch: 96 Loss: 0.1198\n",
      "Epoch: 4, batch: 97 Loss: 0.1198\n",
      "Epoch: 4, batch: 98 Loss: 0.1197\n",
      "Epoch: 4, batch: 99 Loss: 0.4635\n",
      "Epoch: 4, batch: 100 Loss: 0.4636\n",
      "Epoch: 4, batch: 101 Loss: 0.4636\n",
      "Epoch: 4, batch: 102 Loss: 0.4636\n",
      "Epoch: 4, batch: 103 Loss: 0.4636\n",
      "Epoch: 4, batch: 104 Loss: 0.1194\n",
      "Epoch: 4, batch: 105 Loss: 0.1194\n",
      "Epoch: 4, batch: 106 Loss: 0.1193\n",
      "Epoch: 4, batch: 107 Loss: 0.4637\n",
      "Epoch: 4, batch: 108 Loss: 0.4638\n",
      "Epoch: 4, batch: 109 Loss: 0.4638\n",
      "Epoch: 4, batch: 110 Loss: 1.1535\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-b56425c5723a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m50\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[1;31m#print(\"Epoch: {}, batch: {} Loss: {} label_loss:{}\".format(i, batch_idx, loss, label_loss_))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Epoch: {}, batch: {} Loss: {:0.4f}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36m__format__\u001b[1;34m(self, format_spec)\u001b[0m\n\u001b[0;32m    558\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__format__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat_spec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__format__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformat_spec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    561\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__format__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat_spec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training\n",
    "loss_list = []\n",
    "alpha=0.2\n",
    "epochs = 100\n",
    "\n",
    "net.train()\n",
    "\n",
    "for i in range(epochs):\n",
    "    for (batch_idx, batch) in enumerate(trainloader):\n",
    "        XZ_train_batch = batch[0].cuda() # remove .cuda() if you don't have a GPU\n",
    "        YZ_train_batch = batch[1].cuda() # remove .cuda() if you don't have a GPU\n",
    "        sig_train_batch = batch[2].cuda() # remove .cuda() if you don't have a GPU\n",
    "\n",
    "        Netout = net.forward(XZ_train_batch, YZ_train_batch) # This will call the forward function, usually it returns tensors.\n",
    "        #print(F.softmax(Netout))\n",
    "        loss = criterion(Netout, sig_train_batch) # classification loss\n",
    "\n",
    "        \n",
    "        \n",
    "        # Zero the gradients before running the backward pass.\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "        # parameters of the model. Internally, the parameters of each Module are stored\n",
    "        # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "        # all learnable parameters in the model.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Calling the step function on an Optimizer makes an update to its\n",
    "        # parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_list.append(loss)\n",
    "        if batch_idx % 50 == 0 or True:\n",
    "            #print(\"Epoch: {}, batch: {} Loss: {} label_loss:{}\".format(i, batch_idx, loss, label_loss_))\n",
    "            print(\"Epoch: {}, batch: {} Loss: {:0.4f}\".format(i, batch_idx, loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "siglist=[]\n",
    "for items in file_list:\n",
    "    data = np.load( os.path.join( data_path, items ) )\n",
    "    siglist.append([items, data['sig']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siglist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # Prediction\n",
    "    preds = []\n",
    "    reals = []\n",
    "\n",
    "    for (batch_idx, batch) in enumerate(testloader):\n",
    "        XZ_test_batch = batch[0].cuda() # remove .cuda() if you don't have a GPU\n",
    "        YZ_test_batch = batch[1].cuda() # remove .cuda() if you don't have a GPU\n",
    "        sig_test_batch = batch[2].cuda() # remove .cuda() if you don't have a GPU\n",
    "\n",
    "        Netout = net.forward(XZ_test_batch, YZ_test_batch) # This will call the forward function, usually it returns tensors.\n",
    "        #print(Netout.shape)\n",
    "        prediction=F.softmax(Netout, dim=1).argmax(dim=1)\n",
    "        #print(predictor)\n",
    "\n",
    "        preds.append(prediction.cpu().detach().numpy())\n",
    "        reals.append(sig_test_batch.cpu().detach().numpy())\n",
    "    preds=np.array(preds)\n",
    "    reals=np.array(reals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "343"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(preds.flatten()==reals.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiYAAAGbCAYAAADwcltwAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAiqklEQVR4nO3debgeZXn48e+dk7AZIGxCTKKhNC2LlYAh4FaRRRaRWIsIWsQ2NVaBn92x2w+o2l9B1J9URCJwiS0lIjUaFWUruFX2PQEkyJIcAkGWJIIgOdz9450Dx5CcMwnve96Zeb+f65rrzDwz78wz15WTuc99P8+8kZlIkiRVwZhud0CSJGmQgYkkSaoMAxNJklQZBiaSJKkyDEwkSVJljO30BSJOcdqP1A27nNTtHkg9KxcRo3m9UyLa9qw9KXNU+74mMyaSJKkyOp4xkSRJndWkh3mT7kWSpJ40rtsdaCNLOZIkqTLMmEiSVHNNepg36V4kSepJlnIkSZI6wIyJJEk116SHeZPuRZKknmQpR5IkqQPMmEiSVHNNepg36V4kSepJlnIkSZI6wIyJJEk116SHeZPuRZKknmQpR5IkqQPMmEiSVHNNypgYmEiSVHNNephbypEkSZXRpCBLkqSeZClHkiRVRpMe5pZyJElSZTQpyJIkqSdZypEkSZXRpIe5pRxJklQZTQqyJEnqSZZyJElSZTTpYW4pR5IkVUaTgixJknqSpRxJklQZTXqYN+leJEnqSU3KmDjGRJIkVYYZE0mSaq5JGRMDE0mSaq5JD3NLOZIkqTKaFGRJktSTxjXoad6gW5EkqTeNbdDT3FKOJEmqjAbFWJIk9aZxfd3uQfsYmEiSVHOWciRJkjqgQTGWJEm9yVk5kiSpOho0xsRSjiRJqgwDE0mS6m5sG5dhRMQmEXFdRNwaEQsj4pSifceIuDYiFkfE1yJio6J942J7cbF/6ki3YmAiSVLdjVJgAjwL7JeZuwPTgYMjYh/gVOBzmfnbwBPA7OL42cATRfvniuOGZWAiSZJKyZZfFpvjiiWB/YCLi/bzgXcV67OKbYr9+0dEDHcNAxNJkuqujRmTiJgTETcMWeYMvVRE9EXELcBy4HLgXuDJzFxdHLIUmFSsTwKWABT7VwDbjHQrkiSpzto4Kycz5wJzh9k/AEyPiAnAfGDn9l3djIkkSdoAmfkkcBXwBmBCRAwmOyYD/cV6PzAFoNi/JfDYcOc1MJEkqe5Gb1bOdkWmhIjYFDgQuJNWgHJEcdixwLeK9QXFNsX+/87MHOlWJElSnY3e03wicH5E9NFKblyUmd+JiEXAvIj4JHAzcG5x/LnAv0fEYuBx4KiRLmBgIkmSSsnM24A91tL+c2DmWtqfAd6zPtcwMJEkqe4a9Ep6AxNJkuquQU9zB79KkqTKaFCMJUlSj2rQ07xBtyJJUo9q0BgTSzmSJKkyzJhIklR3DXqaN+hWJEnqUQ16mlvKkSRJldGgGEuSpB7VoKd5g25FkqQe5awcSZKk9jNjIklS3TXoad6gW5EkqUc16GluKUeSJFXGsDFWRGw93P7MfLy93ZEkSeutQYNfR0r+3AgkEMCrgSeK9QnAg8COneycJEkqoVdKOZm5Y2b+FnAF8M7M3DYztwEOAy4bjQ5KkqTeUXaMyT6ZecngRmZ+D3hjZ7okSZLWy9g2Ll1WtgsPRcQ/Av9RbL8feKgzXZIkSeulQWNMymZMjga2A+YXyyuLNkmSpLYplTEpZt98rMN9kSRJG6ICJZh2KXUrEfE7wF8DU4d+JjP360y3JElSab0WmABfB74EnAMMdK47kiSpl5UNTFZn5lkd7YkkSdowPZgx+XZEfJTWwNdnBxt986skSRXQoFk5ZQOTY4uffzOkLYHfam93JElSLys7K8dXz0uSVFW9VsqJiA+srT0zv9re7kiSpPXWa4EJsNeQ9U2A/YGbAAMTSZLUNmVLOScM3Y6ICcC8TnRIkiStpx4c/LqmpwDHnUiSVAW9VsqJiG/TmoUDrbhsF+CiTnVKkiSth14LTIDTh6yvBh7IzKUd6I8kSephZceY/CAitufFQbD3dK5LkiRpvTQoYzKmzEERcSRwHfAe4Ejg2og4opMdkyRJJfW1cemysjHWPwB7ZeZygIjYDrgCuLhTHZMkSb2nbGAyZjAoKTxGyWyL6umgg3bi858/mL6+MZxzzk2ceupPut0lqbHuuxxWPQUDz8Pq1bDXkTDvM/C7xdzHCZvDk6tgj3d3t5+qsAaVcsreyvcj4lLgwmL7vcAlnemSum3MmODMMw/lwAP/naVLV3L99R9iwYK7ufPOX3S7a1Jjve2D8NiTL24f9Vcvrp/+t7Bi1Wj3SLXSoMBkxKxHRARwBnA28LpimZuZJ3a4b+qSmTMnsXjx49x335M899zzzJu3kFmzdu52t6SedeRBcKF/CqpHjBhjZWZGxCWZ+XvAN0ahT+qySZM2Z8mSlS9sL126kr33ntTFHknNlgmXndP6efZF8OWvv7jvLa+HRx6DxQ90r3+qgQoMWm2XssmfmyJir8y8vszBETEHmNPaOgyYsUGdk6Re8OY/goeWw3Zbw+XnwF0/hx/d2Np39DvMlqiEXirlFPYGfhoR90bEbRFxe0Tctq6DM3NuZs7IzBkGJfXT37+KKVO2eGF78uQt6O+3wC11ykPF1IJHH4f5V8LM17W2+/rg3QfA177Xvb5Jo61sjHVQR3uhSrn++n6mTduGqVMn0N+/kqOO2o33vc8qntQJm20KYwJ++XRr/e1vhH8+q7XvgDfAXfdB/yPd7aNqoEEZk7K3srY/l/0TuqEGBpLjj7+ESy/9I/r6gvPOu4VFix7tdrekRtp+G5h/Rmt97Fj4z+/CpT9ubR91iGUcldSgMSaRmSMfFHE/MAV4AghgAvAw8Ajwocy8cd2fPWXkC0hqv11O6nYPpJ6Vi4hRveBXon3P2g/m6PZ9DWXHmFwOHJqZ22bmNsAhwHeAjwJf7FTnJElSCWPbuAwjIqZExFURsSgiFkbEx4r2kyOiPyJuKZZDh3zm7yJicUTcHREjDg0pW8rZJzM/NLiRmZdFxOmZ+eGI2LjkOSRJUieM3hiT1cBfZeZNEbE5cGNEXF7s+1xmnj704IjYFTgK2A14FXBFRPxOZg6s6wJlMybLIuLEiHhNsfwtsDwi+oDn1/euJElS/WTmssy8qVhfBdwJDPeiq1nAvMx8NjPvAxYDM4e7RtnA5H3AZOCbwHxa402OpjXc5siS55AkSZ3QxlJORMyJiBuGLHPWdsmImArsAVxbNB1fvFLkvIjYqmibBCwZ8rGlDB/IlE7+bJ6ZJ6zRocEXri0ueQ5JktQJbZyVk5lzgbnDHRMR44H/Av48M1dGxFnAJ4Asfn4G+JMNuX7ZjMl/RcQLEU5E/D5w3oZcUJIk1VdEjKMVlFyQmd8AyMxHMnMgM58HvsyL5Zp+WlWWQZOLtnUqG5h8GPhmROxQjLT9N+DQET4jSZJGw+jNygngXODOzPzskPaJQw77A+COYn0BcFREbBwROwLTgOtGupURZeb1EfF/gMuAZ4ADMtM3bkmSVAWjNyvnTcAxwO0RcUvR9vfA0RExnVYp535aCQ0yc2FEXAQsojWj57jhZuTACLcSEd8uLjJoM2AFcG5EkJmHr+cNSZKkmsrMH8NaXx63zncUZ+angE+VvcZIMdbpI+yXJEnd1qBX0g8bmGTmDwCKutCyzHym2N4U2L7z3ZMkSSNq0Jf4lR38+nV+80VqA0WbJElS25SNscZm5q8HNzLz1xGxUYf6JEmS1kcPZkwejYgXBrpGxCzgF53pkiRJWi99bVy6rGyM9WfABRHxBVqjcZcAH+hYryRJUk8q+x6Te4F9ilfQkpm/7GivJElSeQ0q5ZS+lYh4B62vLd6k9eI3yMx/7lC/JElSWQ0KTEqNMYmILwHvBU6gVcp5D/CaDvZLkiT1oLKDX9+YmR8AnsjMU4A3AL/TuW5JkqTSRum7ckZD2S78qvj5dES8CngMmDjM8ZIkabRUYDZNu5QNTL4TEROA04Abi7ZzOtIjSZLUs8oGJqcDHwHeAvwU+BFwVqc6JUmS1kMFSjDtUvZWzgdWAWcU2+8Dvgoc2YlOSZKk9dCDgclrM3PXIdtXRcSiTnRIkiT1rrKzcm6KiH0GNyJib+CGznRJkiStl155JX1E3A4kMA74n4h4sNh+DXBX57snSZJG1EOlnMNGpReSJEmMEJhk5gOj1RFJkrSBeihjIkmSqq5BT/Oyg18lSZI6rkExliRJvSnbOJsm2neqDWJgIklSzQ208Wne7cDAUo4kSaqMbgdGkiTpZWpSxqTb15ckSS/T6r72FUA2btuZNoylHEmSVBlmTCRJqrmBsc15nDfnTiRJ6lEDfRX49r02sZQjSZIqw4yJJEk1N0BzMiYGJpIk1dxqAxNJklQVAw16nDvGRJIkVUZzQixJknqUY0wkSVJlNCkwsZQjSZIqw4yJJEk116SMiYGJJEk116TpwpZyJElSZZgxkSSp5pr0HpPm3IkkST2qSWNMLOVIkqTKMGMiSVLNNSljYmAiSVLNOStHkiSpA8yYSJJUc02alWPGRJKkmhugr23LcCJiSkRcFRGLImJhRHysaN86Ii6PiHuKn1sV7RERZ0TE4oi4LSL2HOleDEwkSVJZq4G/ysxdgX2A4yJiV+DjwJWZOQ24stgGOASYVixzgLNGukBzcj+SJPWo0ZqVk5nLgGXF+qqIuBOYBMwC9i0OOx+4GjixaP9qZiZwTURMiIiJxXnWysBEkqSaa2dgEhFzaGU3Bs3NzLlrOW4qsAdwLbD9kGDjYWD7Yn0SsGTIx5YWbQYmkiRpZEUQ8pJAZKiIGA/8F/DnmbkyIoZ+PiMiN/T6BiaSJNXcaL7HJCLG0QpKLsjMbxTNjwyWaCJiIrC8aO8Hpgz5+OSibZ0c/CpJUs0NMLZty3CilRo5F7gzMz87ZNcC4Nhi/VjgW0PaP1DMztkHWDHc+BIwYyJJksp7E3AMcHtE3FK0/T3wr8BFETEbeAA4sth3CXAosBh4GvjjkS5gYCJJUs2N4qycHwOxjt37r+X4BI5bn2sYmEiSVHNN+hI/x5hIkqTKMGMiSVLNNenbhQ1MJEmqOb/ET5IkqQOaE2JJktSjmjT41cBEkqSaa1JgYilHkiRVhhkTSZJqzlk5kiSpMpyVI0mS1AHNCbEkSepRTRr8amAiSVLNNSkwsZQjSZIqw4yJJEk116SMiYGJJEk116TpwpZyJElSZZgxkSSp5pr0HpPm3IkkST2qSWNMLOVIkqTKMGMiSVLNNSljYmAiSVLNOStHkiSpA8yYSJJUc87KkSRJldGkMSaWciRJUmWYMZEa6uQ7o9tdkHpYjurVmpQxMTCRJKnmmhSYWMqRJEmVYcZEkqSaa9J7TAxMJEmqOacLS5KkynCMiSRJUgeYMZEkqeaalDExMJEkqeaaNPjVUo4kSaoMMyaSJNWcs3IkSVJlNGmMiaUcSZJUGWZMJEmquSZlTAxMJEmquSYFJpZyJElSZZgxkSSp5pr0HhMDE0mSaq5J04Ut5UiSpMpoToglSVKPcvCrJEmqjAH62raMJCLOi4jlEXHHkLaTI6I/Im4plkOH7Pu7iFgcEXdHxEEjnd/ARJIkrY+vAAevpf1zmTm9WC4BiIhdgaOA3YrPfDEiho1+LOVIklRzozkrJzN/GBFTSx4+C5iXmc8C90XEYmAm8NN1fcCMiSRJNTfA2LYtETEnIm4Ysswp2Y3jI+K2otSzVdE2CVgy5JilRds6GZhIkqQXZObczJwxZJlb4mNnATsB04FlwGc29PqWciRJqrluz8rJzEcG1yPiy8B3is1+YMqQQycXbetkxkSSpJobzVk5axMRE4ds/gEwOGNnAXBURGwcETsC04DrhjuXGRNJklRaRFwI7AtsGxFLgZOAfSNiOpDA/cCHATJzYURcBCwCVgPHZebAcOc3MJEkqeZGeVbO0WtpPneY4z8FfKrs+Q1MJEmqOb8rR5IkqQOaE2JJktSjuj0rp50MTCRJqrkmBSaWciRJUmWYMZEkqeaalDExMJEkqeZGc7pwp1nKkSRJlWHGRJKkmmvSe0yacyeSJPWoJo0xsZQjSZIqw4yJJEk116SMiYGJJEk156wcSZKkDjBjIklSzTkrR5IkVUaTxphYypEkSZVhxkSSpJprUsbEwESSpJobeL6NgUmXaymWciRJUmWYMZEkqeZWr25jxmSj9p1qQxiYSJJUcwOr2/g473JgYilHkiRVhhkTSZJqbqCdpZwuMzCRJKnmmhSYWMqRJEmVYcZEkqSaW/1cczImBiaSJNXc8wPNeZxbypEkSZXRnBBLkqRe1aDBrwYmkiTVnYGJJEmqjNXR7R60jWNMJElSZZgxkSSp7lZ3uwPtY2AiSVLdNSgwsZQjSZIqw4yJJEl116CMiYGJJEl191y3O9A+lnIkSVJlmDGRJKnuBrrdgfYxMJEkqe4aNMbEUo4kSaoMMyaSJNVdgzImBiaSJNVdgwITSzmSJKkyzJhIklR3DcqYGJhIklR3DQpMLOVIkqTSIuK8iFgeEXcMads6Ii6PiHuKn1sV7RERZ0TE4oi4LSL2HOn8BiaSJNXd6jYuI/sKcPAabR8HrszMacCVxTbAIcC0YpkDnDXSyQ1MJEmqu+fauIwgM38IPL5G8yzg/GL9fOBdQ9q/mi3XABMiYuJw5zcwkSRJL4iIORFxw5BlTomPbZ+Zy4r1h4Hti/VJwJIhxy0t2tbJwa+SJNVdG78rJzPnAnNfxuczInJDP29gIklS3XV/Vs4jETExM5cVpZrlRXs/MGXIcZOLtnWylCNJkl6uBcCxxfqxwLeGtH+gmJ2zD7BiSMlnrcyYSJJUd6OYMYmIC4F9gW0jYilwEvCvwEURMRt4ADiyOPwS4FBgMfA08McjnX/YwCQiVgFrqxMFrTLSFuVuQ5IkdcwoBiaZefQ6du2/lmMTOG59zj9sYJKZm6/PySRJkl6O9SrlRMQrgU0GtzPzwbb3SJIkrZ/uD35tm1KBSUQcDnwGeBWtkbavAe4Edutc1yRJUikNCkzKzsr5BLAP8LPM3JFWHemajvVKkiT1pLKlnOcy87GIGBMRYzLzqoj4/53smCRJKqlBGZOygcmTETEe+CFwQUQsB57qXLckSVJpJb7jpi7KlnJmAb8C/gL4PnAv8M5OdUrdd9BBO3HXXcdxzz0ncOKJb+p2d6RK69t4Y/702mv58C238JE77mDfk09e57G7vPvdnJTJxNe//mVfd8LUqcy+5hpOuOce/nDePMaMGwfAPn/xF3x04UL+7NZbOeaKK9jy1a9+2deSRkupwCQzn8rMgcxcnZnnZ+YZmflYpzun7hgzJjjzzEM55JAL2HXXMzn66Neyyy7bdrtbUmUNPPss5++3H2dPn87Z06ez08EHM2nvvV9y3Ebjx7P3xz7G0mvWb4je7scey1tPOukl7QeceirXfO5z/Nu0aTzzxBPsOXs2AA/ffDNzZ8zgS7vvzp0XX8wBp522YTem+hho49JlpQKTiHh3RNwTESsiYmVErIqIlZ3unLpj5sxJLF78OPfd9yTPPfc88+YtZNasnbvdLanSnnuqVd0eM24cfePGQb703ZRv+8Qn+Mmpp7L6mWdeaIsxYzjwtNP40+uu489uvZXXzynzRa4tO+63H4suvhiAW88/n99917sAuP/qq1n9q18BsPSaa9hi8uQNvS3Vxeo2Ll1WtpRzGnB4Zm6ZmVtk5ua+9bW5Jk3anCVLXow7ly5dyaRJvmtPGk6MGcOHb76Zv1m+nJ9ffjn91133G/t32GMPtpgyhXsuueQ32veYPZtnVqzgnJkz+fJee7Hnhz7EhKlTR7zepttswzNPPkkOtP7EXbl0KVtMeum3ye8xezaLv/e9Db8xaZSVHfz6SGbeWfakETEHKML+w4AZ690xSaqTfP55zt5jDzbeckveO38+2+22G48uXNjaGcFBn/0s3/zgB1/yuZ3e/na2f93r2PWIIwDYeMst2XraNJ5duZIPXHklAJtuvTV9G23EzkVGZP4xx7Bq2bDfgwbA773//bxqxgy+8ta3tuUeVWEVyHS0S9nA5IaI+BrwTeDZwcbM/MbaDs7MucBcgIhT1vZdO6qw/v5VTJnyYkJs8uQt6O9f1cUeSfXx7IoV3H/VVfz2wQe/EJhsvPnmvPK1r+WDV18NwPgdduDoBQu48PDDIYLvnXAC91522UvOdfYeewCtMSYTpk7lB6ec8hv7N5kwgejrIwcG2GLyZFb2v/ht8jvuvz9v+Yd/4CtvfSsDv/51h+5WldGgwKRsKWcLWt8K+HZas3HeSSsVoga6/vp+pk3bhqlTJzBu3BiOOmo3Fiy4u9vdkiprs223ZeMttwRg7Cab8FsHHsgv7rrrhf3PrlzJp7fbjs/vuCOf33FHll5zDRcefjjLbryRey+9lBkf+Qhjxrb+Ttx62jTGbbZZqeved9VVL2Radj/2WO7+Vuub5neYPp3Dzj6beYcfztOPPtrOW5U6rlTGJDNH/JpiNcfAQHL88Zdw6aV/RF9fcN55t7Bokf+5SesyfuJE3nX++Yzp6yPGjGHhRRdxz3e/y76nnMJDN9zAz7797XV+9qZzzmHC1KnMuekmIoKnHn2UrxUlm5FcceKJHDFvHvt98pMsu/lmbj73XAAO/PSn2Wj8eN7z9a8DsOLBB5k3a9bLvk9VWIPeYxK5lpHjLzko4oy1NK8AbsjMbw3/WUs5UjeczMnd7oLUs07KjNG8XvwlbXvW5mcZ1b6vqWwpZxNgOnBPsbwOmAzM9tX0kiSpXcoOfn0d8KbMHACIiLOAHwFvBm7vUN8kSVIZDRr8WjYw2QoYT6t8A/AKYOvMHIiIZ9f9MUmS1HE9GJicBtwSEVcDAfw+8C8R8Qrgig71TZIk9Ziys3LOjYhLgJlF099n5kPF+t90pGeSJKmcBs3KGTYwiYidM/OuiNizaFpS/NwhInbIzJs62z1JkjSiCnz5XruMlDH5S1qvlv/MkLahU5L2a3uPJElSzxo2MMnMwa+5PAv4fmaujIh/AvYEPtHpzkmSpBIaNPi17HtM/rEISt5MK0tyDq1gRZIkddvqNi5dVjYwGaxevQP4cmZ+F9ioM12SJEm9qux04f6IOBs4EDg1IjamfFAjSZI6qUGzcsoGF0cClwIHZeaTwNY4TViSpGoYaOPSZWXfY/I08I0h28uAZZ3qlCRJ6k1lSzmSJKmqKjBotV0MTCRJqjsDE0mSVBk9OPhVkiSp48yYSJJUdxWYTdMuBiaSJNVdg8aYWMqRJEmVYcZEkqS6a1DGxMBEkqS6c1aOJElS+5kxkSSp7pyVI0mSKqNBY0ws5UiSpMowYyJJUt01KGNiYCJJUt05K0eSJKn9zJhIklR3zsqRJEmV4RgTSZLUiyLifmAVrTzN6sycERFbA18DpgL3A0dm5hMbcn7HmEiSVHer27iU87bMnJ6ZM4rtjwNXZuY04Mpie4OYMZEkqe66PytnFrBvsX4+cDVw4oacyIyJJEl6QUTMiYgbhixz1jgkgcsi4sYh+7bPzGXF+sPA9ht6fTMmkiTVXRtn5WTmXGDuMIe8OTP7I+KVwOURcdcan8+IyA29voGJJEl1N4qzcjKzv/i5PCLmAzOBRyJiYmYui4iJwPINPb+lHEmSVEpEvCIiNh9cB94O3AEsAI4tDjsW+NaGXsOMiSRJdTd6GZPtgfkRAa0Y4j8z8/sRcT1wUUTMBh4AjtzQCxiYSJJUd6M0Kyczfw7svpb2x4D923ENSzmSJKkyzJhIklR3fleOJEmqjA2enFs9lnIkSVJlGJhIkqTKMDCRJEmVYWAiSZIqw8BEkiRVhrNyJEmqvXa+YW1cG8+1/syYSJKkyjBjIklS7bXzy3K6mzExMJEkqfbaWcrZtI3nWn+WciRJUmWYMZEkqfbaWcrpLgMTSZJqr52lnO6ylCNJkirDjIkkSbXXnIyJgYkkSbXXnDEmlnIkSVJlmDGRJKn2LOVIkqTKsJQjSZLUdmZMJEmqPUs5kiSpMizlSJIktZ0ZE0mSas9SjiRJqgxLOZIkSW1nxkSSpNqzlCNJkiqjOaUcAxNJkmqvORkTx5hIkqTKMGMiSVLtWcqRJEmVYSlHkiSp7cyYSJJUe83JmBiYSJJUe80ZY2IpR5IkVYYZE0mSas9SjiRJqgxLOZIkSW1nxkSSpNqzlCNJkirDUo4kSVLbmTGRJKn2LOVIkqTKsJQjSZLUdgYmkiTV3nNtXIYXEQdHxN0RsTgiPt7uO7GUI0lS7Y1OKSci+oAzgQOBpcD1EbEgMxe16xpmTCRJUlkzgcWZ+fPM/DUwD5jVzgt0PGOSeVJ0+hrqnIiYk5lzu90PbYiTut0BvQz+7ml9tPNZGxFzgDlDmuYO+bc4CVgyZN9SYO92XRvMmGhkc0Y+RFIH+LunrsjMuZk5Y8gyqgGygYkkSSqrH5gyZHty0dY2BiaSJKms64FpEbFjRGwEHAUsaOcFnJWjkVjjlrrD3z1VTmaujojjgUuBPuC8zFzYzmtEZrbzfJIkSRvMUo4kSaoMAxNJklQZBiY1EhFTI+KOl3mOfSPiO+3qUztFxP0RsW23+yF1WkScExG7duC8v2z3OaXR5uBXlRYRQWtc0vPd7otUZ5n5p93ug1RVZkzqZ2xEXBARd0bExRGxWUT834i4PiLuiIi5RQBBRPx2RFwREbdGxE0RsdPQE0XEXhFxc0TsFBHbRcTlEbGw+GvugYjYtsjS3B0RXwXuAKZExKeLa90eEe8tzvUbmZiI+EJEfLBYvz8iTin6cHtE7Fy0bxMRlw1eE/AtwWqciHhFRHy3+D28IyLeGxFXR8SMYv/siPhZRFwXEV+OiC8U7V+JiDMi4n8i4ucRcUTRPj4irhzy+9TW14FL3WZgUj+/C3wxM3cBVgIfBb6QmXtl5muBTYHDimMvAM7MzN2BNwLLBk8SEW8EvgTMysx7ab2//L8zczfgYuDVQ645rbjmbsAMYDqwO3AA8OmImFii37/IzD2Bs4C/LtpOAn5cnHf+GteUmuJg4KHM3L34Hf3+4I6IeBXwT8A+wJuAndf47ETgzbR+p/+1aHsG+IPi9+ltwGcG/xiRmsDApH6WZOZPivX/oPWf1tsi4tqIuB3YD9gtIjYHJmXmfIDMfCYzny4+twutdyS8MzMfLNreTOvLmMjM7wNPDLnmA5l5zZDjLszMgcx8BPgBsFeJfn+j+HkjMLVY//3iHsjM765xTakpbgcOjIhTI+ItmbliyL6ZwA8y8/HMfA74+hqf/WZmPl98c+v2RVsA/xIRtwFX0Pruku2RGsIxJvWz5otnEvgiMCMzl0TEycAmI5xjWXHMHsBDJa75VIljVvObge6afXi2+DmA/+7UQzLzZxGxJ3Ao8MmIuHI9Pv7skPXBrMj7ge2A12fmcxFxPyP/zku1Ycakfl4dEW8o1t8H/LhY/0VEjAeOAMjMVcDSiHgXQERsHBGbFcc+CbwD+H8RsW/R9hPgyOLYtwNbreP6PwLeGxF9EbEdrazHdcADwK7FdSYA+5e4lx8W90BEHDLMNaXaKso1T2fmfwCfBvYcsvt64K0RsVVEjAX+sMQptwSWF0HJ24DXtL3TUhf5l2v93A0cFxHnAYtojdnYitbA1Idp/Uc36Bjg7Ij4Z+A54D2DOzLzkYg4DPheRPwJcApwYUQcA/y0ONcqYPwa158PvAG4lVa25m8z82GAiLio6Md9wM0l7mXwmguB/wEeHOF4qY5+j9ZYrOdp/R5+BDgdIDP7I+JfaAX3jwN3ASvWdaLCBcC3i9LtDcVnpMbwlfQCWhkVYKD4HoQ3AGdl5vQud0tqvIgYn5m/LDIm82l998j8bvdL6hYzJhr0auCiiBgD/Br4UJf7I/WKkyPiAFrjRC4Dvtnd7kjdZcZEkiRVhoNfJUlSZRiYSJKkyjAwkSRJlWFgIkmSKsPARJIkVcb/AlFkJtML7d8wAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_cm = pd.DataFrame(confusion_matrix(reals.flatten(), preds.flatten()), index = ['background', 'signal'],\n",
    "                  columns = ['background', 'signal'])\n",
    "plt.figure(figsize = (10,7))\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "sn.heatmap(df_cm, annot=True, cmap=\"jet\")\n",
    "plt.show()\n",
    "\n",
    "# Confusion matrix whose i-th row and j-th column entry indicates the number of samples with true label being i-th class and predicted label being j-th class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "343"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(reals.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net, './model_save_train.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.load('./model_save_train.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 45,  12],\n",
       "       [  8, 335]], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(reals.flatten(), preds.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6e343a66b0d3f3fb9e5b3006acd45e89d57a985b4e0912ddff9600a29bb2e852"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
