{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from os import listdir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = ['fd-1kpc-9.6sm-0-overlaid.csv/npz_subevents']\n",
    "file_list = []\n",
    "for path_ in data_path:\n",
    "    for file_ in listdir(path_):\n",
    "        file_list.append(os.path.join(path_, file_))\n",
    "file_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208\n",
      "192\n"
     ]
    }
   ],
   "source": [
    "siglist=[]\n",
    "for items in file_list:\n",
    "    data = np.load( items )\n",
    "    siglist.append(data['sig'])\n",
    "\n",
    "sigindex=np.array(file_list, dtype='U')[np.array(siglist, dtype=int)==1]\n",
    "bkgindex=np.array(file_list, dtype='U')[np.array(siglist, dtype=int)==0]\n",
    "print(len(sigindex))\n",
    "print(len(bkgindex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, file_list_):\n",
    "        self.file_list = file_list_\n",
    "        self.len = len(file_list_)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        file_name = self.file_list[index]\n",
    "        data = np.load( file_name ) \n",
    "        return torch.from_numpy(data['imxz'][:, :, :]).to(torch.float), torch.from_numpy(data['imyz'][:, :, :]).to(torch.float), torch.from_numpy(data['sig']).to(torch.float)\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "mydataset = MyDataset(file_list)\n",
    "batch_size_train = 6\n",
    "batch_size_test = 2\n",
    "\n",
    "\n",
    "train_size = int(0.8 * len(mydataset))\n",
    "test_size = len(mydataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(mydataset, [train_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size_train,\n",
    "                                            shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size_test,\n",
    "                                            shuffle=False)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1_1 = nn.Conv2d(2, 64, 5) \n",
    "        self.conv1_2 = nn.Conv2d(2, 64, 5) \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2_1 = nn.Conv2d(64, 128, 5)\n",
    "        self.conv2_2 = nn.Conv2d(64, 128, 5)\n",
    "        self.conv3_1 = nn.Conv2d(128, 256, 6)\n",
    "        self.conv3_2 = nn.Conv2d(128, 256, 6)\n",
    "\n",
    "        self.fc1_1 = nn.Linear(256 * 52 * 44, 50)\n",
    "        self.fc1_2 = nn.Linear(256 * 52 * 44, 50)\n",
    "        self.fc2= nn.Linear(100, 1)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # x1, x2 shape: (896, 384)      channel = 2\n",
    "        x1 = self.pool(F.relu(self.conv1_1(x1))) # shape: (448, 384)->(444, 380)->(222, 190)\n",
    "        x1 = self.pool(F.relu(self.conv2_1(x1))) # shape: (222, 190)->(218, 186)->(109, 93)\n",
    "        x1 = self.pool(F.relu(self.conv3_1(x1))) # shape: (109, 93)->(104, 88)->(52, 44)\n",
    "        x1 = torch.flatten(x1, 1) # flatten all dimensions except batch \n",
    "        x1 = F.relu(self.fc1_1(x1))\n",
    "\n",
    "        x2 = self.pool(F.relu(self.conv1_2(x2))) # shape: (448, 384)->(444, 380)->(222, 190)\n",
    "        x2 = self.pool(F.relu(self.conv2_2(x2))) # shape: (222, 190)->(218, 186)->(109, 93)\n",
    "        x2 = self.pool(F.relu(self.conv3_2(x2))) # shape: (109, 93)->(104, 88)->(52, 44)\n",
    "        x2 = torch.flatten(x2, 1) # flatten all dimensions except batch \n",
    "        x2 = F.relu(self.fc1_2(x2))\n",
    "\n",
    "        return self.fc2(torch.cat((x1, x2), 1))\n",
    "net=Net().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61349193"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce GTX 1660 Ti'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "loss_list = []\n",
    "alpha=0.2\n",
    "epochs = 100\n",
    "\n",
    "\n",
    "accuracy_list = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    net.train()\n",
    "    for (batch_idx, batch) in enumerate(trainloader):\n",
    "        XZ_train_batch = batch[0].cuda() # remove .cuda() if you don't have a GPU\n",
    "        YZ_train_batch = batch[1].cuda() # remove .cuda() if you don't have a GPU\n",
    "        sig_train_batch = batch[2].cuda() # remove .cuda() if you don't have a GPU\n",
    "\n",
    "        Netout = net.forward(XZ_train_batch, YZ_train_batch) # This will call the forward function, usually it returns tensors.\n",
    "        #print(F.softmax(Netout))\n",
    "        loss = criterion(Netout, sig_train_batch) # classification loss\n",
    "        \n",
    "        # Zero the gradients before running the backward pass.\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "        # parameters of the model. Internally, the parameters of each Module are stored\n",
    "        # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "        # all learnable parameters in the model.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Calling the step function on an Optimizer makes an update to its\n",
    "        # parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_list.append(loss)\n",
    "        if batch_idx % 50 == 0 or True:\n",
    "            #print(\"Epoch: {}, batch: {} Loss: {} label_loss:{}\".format(i, batch_idx, loss, label_loss_))\n",
    "            print(\"Epoch: {}, batch: {} Loss: {:0.4f}\".format(i, batch_idx, loss))\n",
    "    \n",
    "    net.eval() # begin testing\n",
    "    preds = np.array([])\n",
    "    reals = np.array([])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (batch_idx, batch) in enumerate(testloader):\n",
    "            XZ_test_batch = batch[0].cuda() # remove .cuda() if you don't have a GPU\n",
    "            YZ_test_batch = batch[1].cuda() # remove .cuda() if you don't have a GPU\n",
    "            sig_test_batch = batch[2].cuda() # remove .cuda() if you don't have a GPU\n",
    "\n",
    "            Netout = net.forward(XZ_test_batch, YZ_test_batch) # This will call the forward function, usually it returns tensors.\n",
    "            #print(Netout.shape)\n",
    "            prediction=Netout\n",
    "            \n",
    "\n",
    "            preds=np.concatenate((preds, prediction.cpu().detach().numpy().flatten()))\n",
    "            reals=np.concatenate((reals, sig_test_batch.cpu().detach().numpy().flatten()))\n",
    "        preds=np.array(preds)\n",
    "        reals=np.array(reals)\n",
    "        accuracy=np.mean((preds-reals)**2)\n",
    "        accuracy_list.append(accuracy)\n",
    "        print(\"Test accuracy: {}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siglist=[]\n",
    "for items in file_list:\n",
    "    data = np.load( os.path.join( data_path, items ) )\n",
    "    siglist.append([items, data['sig']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siglist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "preds = []\n",
    "reals = []\n",
    "\n",
    "for (batch_idx, batch) in enumerate(testloader):\n",
    "    XZ_test_batch = batch[0].cuda() # remove .cuda() if you don't have a GPU\n",
    "    YZ_test_batch = batch[1].cuda() # remove .cuda() if you don't have a GPU\n",
    "    sig_test_batch = batch[2].cuda() # remove .cuda() if you don't have a GPU\n",
    "\n",
    "    Netout = net.forward(XZ_test_batch, YZ_test_batch) # This will call the forward function, usually it returns tensors.\n",
    "    #print(Netout.shape)\n",
    "    prediction=Netout\n",
    "    #print(predictor)\n",
    "\n",
    "    preds.append(prediction.cpu().detach().numpy())\n",
    "    \n",
    "    reals.append(sig_test_batch.cpu().detach().numpy())\n",
    "preds=np.array(preds)\n",
    "reals=np.array(reals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_cm = pd.DataFrame(confusion_matrix(reals.flatten(), preds.flatten()), index = ['background', 'signal'],\n",
    "                  columns = ['background', 'signal'])\n",
    "plt.figure(figsize = (10,7))\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "sn.heatmap(df_cm, annot=True, cmap=\"jet\")\n",
    "plt.show()\n",
    "\n",
    "# Confusion matrix whose i-th row and j-th column entry indicates the number of samples with true label being i-th class and predicted label being j-th class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(reals.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net, './model_save_train_VGG_reduced_time.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.load('./model_save_train_VGG_reduced_time.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 45,  12],\n",
       "       [  8, 335]], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(reals.flatten(), preds.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.40362734],\n",
       "        [0.40550905]],\n",
       "\n",
       "       [[0.22553435],\n",
       "        [0.40490067]],\n",
       "\n",
       "       [[0.39987746],\n",
       "        [0.40550905]],\n",
       "\n",
       "       [[0.39987746],\n",
       "        [0.40550905]],\n",
       "\n",
       "       [[0.39987746],\n",
       "        [0.40613887]],\n",
       "\n",
       "       [[0.4067989 ],\n",
       "        [0.40631136]],\n",
       "\n",
       "       [[0.22553435],\n",
       "        [0.40345606]],\n",
       "\n",
       "       [[0.4066822 ],\n",
       "        [0.4066822 ]],\n",
       "\n",
       "       [[0.22553435],\n",
       "        [0.40550905]],\n",
       "\n",
       "       [[0.40631136],\n",
       "        [0.40550905]],\n",
       "\n",
       "       [[0.40362734],\n",
       "        [0.4067989 ]],\n",
       "\n",
       "       [[0.40631136],\n",
       "        [0.40550905]],\n",
       "\n",
       "       [[0.40550905],\n",
       "        [0.22553435]],\n",
       "\n",
       "       [[0.40345606],\n",
       "        [0.40631136]],\n",
       "\n",
       "       [[0.40550905],\n",
       "        [0.40613887]],\n",
       "\n",
       "       [[0.40490067],\n",
       "        [0.40490067]],\n",
       "\n",
       "       [[0.4067989 ],\n",
       "        [0.4067989 ]],\n",
       "\n",
       "       [[0.40490067],\n",
       "        [0.40550905]],\n",
       "\n",
       "       [[0.40345606],\n",
       "        [0.40490067]],\n",
       "\n",
       "       [[0.40345606],\n",
       "        [0.4067989 ]],\n",
       "\n",
       "       [[0.40631136],\n",
       "        [0.39987746]],\n",
       "\n",
       "       [[0.40631136],\n",
       "        [0.40345606]],\n",
       "\n",
       "       [[0.40613887],\n",
       "        [0.40613887]],\n",
       "\n",
       "       [[0.40613887],\n",
       "        [0.40490067]],\n",
       "\n",
       "       [[0.4066822 ],\n",
       "        [0.4067989 ]],\n",
       "\n",
       "       [[0.40490067],\n",
       "        [0.40613887]],\n",
       "\n",
       "       [[0.40345606],\n",
       "        [0.40550905]],\n",
       "\n",
       "       [[0.40362734],\n",
       "        [0.39987746]],\n",
       "\n",
       "       [[0.39987746],\n",
       "        [0.4066822 ]],\n",
       "\n",
       "       [[0.40345606],\n",
       "        [0.4067989 ]],\n",
       "\n",
       "       [[0.4066822 ],\n",
       "        [0.4066822 ]],\n",
       "\n",
       "       [[0.40550905],\n",
       "        [0.4066822 ]],\n",
       "\n",
       "       [[0.40613887],\n",
       "        [0.39987746]],\n",
       "\n",
       "       [[0.40362734],\n",
       "        [0.22553435]],\n",
       "\n",
       "       [[0.4066822 ],\n",
       "        [0.40362734]],\n",
       "\n",
       "       [[0.40490067],\n",
       "        [0.40345606]],\n",
       "\n",
       "       [[0.40362734],\n",
       "        [0.4067989 ]],\n",
       "\n",
       "       [[0.40345606],\n",
       "        [0.39987746]],\n",
       "\n",
       "       [[0.4067989 ],\n",
       "        [0.40613887]],\n",
       "\n",
       "       [[0.4067989 ],\n",
       "        [0.40550905]]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6e343a66b0d3f3fb9e5b3006acd45e89d57a985b4e0912ddff9600a29bb2e852"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
