{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from os import listdir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './npy0/'\n",
    "file_list = listdir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_path_, file_list_):\n",
    "        self.data_path = data_path_\n",
    "        self.file_list = file_list_\n",
    "        self.len = len(file_list_)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        file_name = self.file_list[index]\n",
    "        data = np.load( os.path.join( self.data_path, file_name ) )\n",
    "        return torch.from_numpy(data['imxz'][None, :, :]).to(torch.float)/4096, torch.from_numpy(data['imyz'][None, :, :]).to(torch.float)/4096, torch.from_numpy(data['sig']).to(torch.long)\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "mydataset = MyDataset(data_path, file_list)\n",
    "batch_size_train = 2\n",
    "batch_size_test = 4\n",
    "\n",
    "\n",
    "train_size = int(0.8 * len(mydataset))\n",
    "test_size = len(mydataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(mydataset, [train_size, test_size])\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size_train,\n",
    "                                            shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size_test,\n",
    "                                            shuffle=False)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1600"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1_1 = nn.Conv2d(1, 64, 5) \n",
    "        self.conv1_2 = nn.Conv2d(1, 64, 5) \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2_1 = nn.Conv2d(64, 128, 5)\n",
    "        self.conv2_2 = nn.Conv2d(64, 128, 5)\n",
    "        self.conv3_1 = nn.Conv2d(128, 256, 6)\n",
    "        self.conv3_2 = nn.Conv2d(128, 256, 6)\n",
    "\n",
    "        self.fc1_1 = nn.Linear(256 * 108 * 44, 50)\n",
    "        self.fc1_2 = nn.Linear(256 * 108 * 44, 50)\n",
    "        self.fc2= nn.Linear(100, 2)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # x1, x2 shape: (896, 384)      channel = 1\n",
    "        x1 = self.pool(F.relu(self.conv1_1(x1))) # shape: (896, 384)->(892, 380)->(446, 190)\n",
    "        x1 = self.pool(F.relu(self.conv2_1(x1))) # shape: (446, 190)->(442, 186)->(221, 93)\n",
    "        x1 = self.pool(F.relu(self.conv3_1(x1))) # shape: (221, 93)->(216, 88)->(108, 44)\n",
    "        x1 = torch.flatten(x1, 1) # flatten all dimensions except batch \n",
    "        x1 = F.relu(self.fc1_1(x1))\n",
    "\n",
    "        x2 = self.pool(F.relu(self.conv1_2(x2))) # shape: (896, 384)->(892, 380)->(446, 190)\n",
    "        x2 = self.pool(F.relu(self.conv2_2(x2))) # shape: (446, 190)->(442, 186)->(221, 93)\n",
    "        x2 = self.pool(F.relu(self.conv3_2(x2))) # shape: (221, 93)->(216, 88)->(108, 44)\n",
    "        x2 = torch.flatten(x2, 1) # flatten all dimensions except batch \n",
    "        x2 = F.relu(self.fc1_2(x2))\n",
    "\n",
    "        return self.fc2(torch.cat((x1, x2), 1))\n",
    "net=Net().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion_weighted = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124424494"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce GTX 1660 Ti'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 166.00 MiB (GPU 0; 6.00 GiB total capacity; 4.10 GiB already allocated; 0 bytes free; 4.28 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-cf74e04042d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0msig_train_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# remove .cuda() if you don't have a GPU\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mNetout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXZ_train_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYZ_train_batch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# This will call the forward function, usually it returns tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[1;31m#print(F.softmax(Netout))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-933ad7110fb2>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x1, x2)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m# x1, x2 shape: (896, 384)      channel = 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mx1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# shape: (896, 384)->(892, 380)->(446, 190)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mx1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# shape: (446, 190)->(442, 186)->(221, 93)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mx1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv3_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# shape: (221, 93)->(216, 88)->(108, 44)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 443\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    437\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m--> 439\u001b[1;33m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    440\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 166.00 MiB (GPU 0; 6.00 GiB total capacity; 4.10 GiB already allocated; 0 bytes free; 4.28 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "loss_list = []\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "accuracy_list = []\n",
    "\n",
    "for i in range(epochs):\n",
    "\n",
    "    net.train() # begin training\n",
    "\n",
    "    for (batch_idx, batch) in enumerate(trainloader):\n",
    "        XZ_train_batch = batch[0].cuda() # remove .cuda() if you don't have a GPU\n",
    "        YZ_train_batch = batch[1].cuda() # remove .cuda() if you don't have a GPU\n",
    "        sig_train_batch = batch[2].cuda() # remove .cuda() if you don't have a GPU\n",
    "\n",
    "        Netout = net.forward(XZ_train_batch, YZ_train_batch) # This will call the forward function, usually it returns tensors.\n",
    "        #print(F.softmax(Netout))\n",
    "\n",
    "\n",
    "        loss = criterion_weighted(Netout, sig_train_batch) # classification loss        \n",
    "        \n",
    "        # Zero the gradients before running the backward pass.\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "        # parameters of the model. Internally, the parameters of each Module are stored\n",
    "        # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "        # all learnable parameters in the model.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Calling the step function on an Optimizer makes an update to its\n",
    "        # parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_list.append(loss)\n",
    "        if batch_idx % 50 == 0 or True:\n",
    "            #print(\"Epoch: {}, batch: {} Loss: {} label_loss:{}\".format(i, batch_idx, loss, label_loss_))\n",
    "            print(\"Epoch: {}, batch: {} Loss: {:0.4f}\".format(i, batch_idx, loss))\n",
    "    \n",
    "    net.eval() # begin testing\n",
    "    preds = np.array([])\n",
    "    reals = np.array([])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (batch_idx, batch) in enumerate(testloader):\n",
    "            XZ_test_batch = batch[0].cuda() # remove .cuda() if you don't have a GPU\n",
    "            YZ_test_batch = batch[1].cuda() # remove .cuda() if you don't have a GPU\n",
    "            sig_test_batch = batch[2].cuda() # remove .cuda() if you don't have a GPU\n",
    "\n",
    "            Netout = net.forward(XZ_test_batch, YZ_test_batch) # This will call the forward function, usually it returns tensors.\n",
    "            #print(Netout.shape)\n",
    "            prediction=F.softmax(Netout, dim=1).argmax(dim=1)\n",
    "            \n",
    "\n",
    "            preds=np.concatenate((preds, prediction.cpu().detach().numpy().flatten()))\n",
    "            reals=np.concatenate((reals, sig_test_batch.cpu().detach().numpy().flatten()))\n",
    "        preds=np.array(preds)\n",
    "        reals=np.array(reals)\n",
    "        accuracy=np.mean(preds==reals)\n",
    "        accuracy_list.append(accuracy)\n",
    "        print(\"Test accuracy: {}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'accuracy_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-2dfa1fdd2980>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0maccuracy_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'accuracy_list' is not defined"
     ]
    }
   ],
   "source": [
    "accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\swyx2\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    net.eval()\n",
    "    # Prediction\n",
    "    preds = np.array([])\n",
    "    reals = np.array([])\n",
    "\n",
    "    for (batch_idx, batch) in enumerate(testloader):\n",
    "        XZ_test_batch = batch[0].cuda() # remove .cuda() if you don't have a GPU\n",
    "        YZ_test_batch = batch[1].cuda() # remove .cuda() if you don't have a GPU\n",
    "        sig_test_batch = batch[2].cuda() # remove .cuda() if you don't have a GPU\n",
    "\n",
    "        Netout = net.forward(XZ_test_batch, YZ_test_batch) # This will call the forward function, usually it returns tensors.\n",
    "        #print(Netout.shape)\n",
    "        prediction=F.softmax(Netout, dim=1).argmax(dim=1)\n",
    "        \n",
    "\n",
    "        preds=np.concatenate((preds, prediction.cpu().detach().numpy().flatten()))\n",
    "        #preds=np.ones((400))\n",
    "        reals=np.concatenate((reals, sig_test_batch.cpu().detach().numpy().flatten()))\n",
    "    preds=np.array(preds)\n",
    "    reals=np.array(reals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_cm = pd.DataFrame(confusion_matrix(reals.flatten(), preds.flatten()), index = ['background', 'signal'],\n",
    "                  columns = ['background', 'signal'])\n",
    "plt.figure(figsize = (10,7))\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "sn.heatmap(df_cm, annot=True, cmap=\"jet\")\n",
    "plt.show()\n",
    "\n",
    "# Confusion matrix whose i-th row and j-th column entry indicates the number of samples with true label being i-th class and predicted label being j-th class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "349"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(reals==preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net, './model_save_train.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.load('./model_save_train.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6e343a66b0d3f3fb9e5b3006acd45e89d57a985b4e0912ddff9600a29bb2e852"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
