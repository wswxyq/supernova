{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from os import listdir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = ['fd-1kpc-9.6sm-0-overlaid.csv/npz_subevents']\n",
    "file_list = []\n",
    "for path_ in data_path:\n",
    "    for file_ in listdir(path_):\n",
    "        file_list.append(os.path.join(path_, file_))\n",
    "file_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "906\n",
      "3094\n"
     ]
    }
   ],
   "source": [
    "siglist=[]\n",
    "for items in file_list:\n",
    "    data = np.load( items )\n",
    "    siglist.append(data['sig'])\n",
    "\n",
    "sigindex=np.array(file_list, dtype='U')[np.array(siglist, dtype=int)!=0]\n",
    "bkgindex=np.array(file_list, dtype='U')[np.array(siglist, dtype=int)==0]\n",
    "print(len(sigindex))\n",
    "print(len(bkgindex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, file_list_):\n",
    "        self.file_list = file_list_\n",
    "        self.len = len(file_list_)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        file_name = self.file_list[index]\n",
    "        data = np.load( file_name ) \n",
    "        return torch.from_numpy(data['imxz'][:, :, :]).to(torch.float), torch.from_numpy(data['imyz'][:, :, :]).to(torch.float), torch.from_numpy(data['sig']).to(torch.float)\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "mydataset = MyDataset(file_list)\n",
    "batch_size_train = 6\n",
    "batch_size_test = 2\n",
    "\n",
    "\n",
    "train_size = int(0.8 * len(mydataset))\n",
    "test_size = len(mydataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(mydataset, [train_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size_train,\n",
    "                                            shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size_test,\n",
    "                                            shuffle=False)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1_1 = nn.Conv2d(2, 64, 5) \n",
    "        self.conv1_2 = nn.Conv2d(2, 64, 5) \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2_1 = nn.Conv2d(64, 128, 5)\n",
    "        self.conv2_2 = nn.Conv2d(64, 128, 5)\n",
    "        self.conv3_1 = nn.Conv2d(128, 256, 6)\n",
    "        self.conv3_2 = nn.Conv2d(128, 256, 6)\n",
    "\n",
    "        self.fc1_1 = nn.Linear(256 * 52 * 44, 50)\n",
    "        self.fc1_2 = nn.Linear(256 * 52 * 44, 50)\n",
    "        self.fc2= nn.Linear(100, 1)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # x1, x2 shape: (896, 384)      channel = 2\n",
    "        x1 = self.pool(F.relu(self.conv1_1(x1))) # shape: (448, 384)->(444, 380)->(222, 190)\n",
    "        x1 = self.pool(F.relu(self.conv2_1(x1))) # shape: (222, 190)->(218, 186)->(109, 93)\n",
    "        x1 = self.pool(F.relu(self.conv3_1(x1))) # shape: (109, 93)->(104, 88)->(52, 44)\n",
    "        x1 = torch.flatten(x1, 1) # flatten all dimensions except batch \n",
    "        x1 = F.relu(self.fc1_1(x1))\n",
    "\n",
    "        x2 = self.pool(F.relu(self.conv1_2(x2))) # shape: (448, 384)->(444, 380)->(222, 190)\n",
    "        x2 = self.pool(F.relu(self.conv2_2(x2))) # shape: (222, 190)->(218, 186)->(109, 93)\n",
    "        x2 = self.pool(F.relu(self.conv3_2(x2))) # shape: (109, 93)->(104, 88)->(52, 44)\n",
    "        x2 = torch.flatten(x2, 1) # flatten all dimensions except batch \n",
    "        x2 = F.relu(self.fc1_2(x2))\n",
    "\n",
    "        return self.fc2(torch.cat((x1, x2), 1))\n",
    "net=Net().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61349193"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce GTX 1660 Ti'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, batch: 0 Loss: 2.3630\n",
      "Epoch: 0, batch: 1 Loss: 7.5779\n",
      "Epoch: 0, batch: 2 Loss: 2.8644\n",
      "Epoch: 0, batch: 3 Loss: 0.9500\n",
      "Epoch: 0, batch: 4 Loss: 2.4372\n",
      "Epoch: 0, batch: 5 Loss: 4.3609\n",
      "Epoch: 0, batch: 6 Loss: 1.3720\n",
      "Epoch: 0, batch: 7 Loss: 1.3370\n",
      "Epoch: 0, batch: 8 Loss: 1.2771\n",
      "Epoch: 0, batch: 9 Loss: 4.9159\n",
      "Epoch: 0, batch: 10 Loss: 2.2180\n",
      "Epoch: 0, batch: 11 Loss: 0.9365\n",
      "Epoch: 0, batch: 12 Loss: 4.0240\n",
      "Epoch: 0, batch: 13 Loss: 0.9209\n",
      "Epoch: 0, batch: 14 Loss: 0.7764\n",
      "Epoch: 0, batch: 15 Loss: 7.0546\n",
      "Epoch: 0, batch: 16 Loss: 0.5043\n",
      "Epoch: 0, batch: 17 Loss: 5.0026\n",
      "Epoch: 0, batch: 18 Loss: 0.4753\n",
      "Epoch: 0, batch: 19 Loss: 0.6559\n",
      "Epoch: 0, batch: 20 Loss: 0.3741\n",
      "Epoch: 0, batch: 21 Loss: 0.6122\n",
      "Epoch: 0, batch: 22 Loss: 0.2744\n",
      "Epoch: 0, batch: 23 Loss: 7.2458\n",
      "Epoch: 0, batch: 24 Loss: 8.6295\n",
      "Epoch: 0, batch: 25 Loss: 2.2578\n",
      "Epoch: 0, batch: 26 Loss: 1.2506\n",
      "Epoch: 0, batch: 27 Loss: 0.2251\n",
      "Epoch: 0, batch: 28 Loss: 0.2301\n",
      "Epoch: 0, batch: 29 Loss: 0.2257\n",
      "Epoch: 0, batch: 30 Loss: 0.2204\n",
      "Epoch: 0, batch: 31 Loss: 2.6451\n",
      "Epoch: 0, batch: 32 Loss: 0.2330\n",
      "Epoch: 0, batch: 33 Loss: 0.5879\n",
      "Epoch: 0, batch: 34 Loss: 4.7082\n",
      "Epoch: 0, batch: 35 Loss: 1.2777\n",
      "Epoch: 0, batch: 36 Loss: 7.4702\n",
      "Epoch: 0, batch: 37 Loss: 3.6357\n",
      "Epoch: 0, batch: 38 Loss: 0.2178\n",
      "Epoch: 0, batch: 39 Loss: 0.9244\n",
      "Epoch: 0, batch: 40 Loss: 0.2396\n",
      "Epoch: 0, batch: 41 Loss: 0.2460\n",
      "Epoch: 0, batch: 42 Loss: 0.2490\n",
      "Epoch: 0, batch: 43 Loss: 0.2484\n",
      "Epoch: 0, batch: 44 Loss: 5.5917\n",
      "Epoch: 0, batch: 45 Loss: 0.2483\n",
      "Epoch: 0, batch: 46 Loss: 0.2474\n",
      "Epoch: 0, batch: 47 Loss: 0.2441\n",
      "Epoch: 0, batch: 48 Loss: 0.5795\n",
      "Epoch: 0, batch: 49 Loss: 0.5776\n",
      "Epoch: 0, batch: 50 Loss: 0.2255\n",
      "Epoch: 0, batch: 51 Loss: 0.2287\n",
      "Epoch: 0, batch: 52 Loss: 3.3097\n",
      "Epoch: 0, batch: 53 Loss: 1.2523\n",
      "Epoch: 0, batch: 54 Loss: 0.2012\n",
      "Epoch: 0, batch: 55 Loss: 15.2054\n",
      "Epoch: 0, batch: 56 Loss: 0.5695\n",
      "Epoch: 0, batch: 57 Loss: 0.5713\n",
      "Epoch: 0, batch: 58 Loss: 27.8078\n",
      "Epoch: 0, batch: 59 Loss: 2.5948\n",
      "Epoch: 0, batch: 60 Loss: 0.5883\n",
      "Epoch: 0, batch: 61 Loss: 1.5600\n",
      "Epoch: 0, batch: 62 Loss: 3.5466\n",
      "Epoch: 0, batch: 63 Loss: 0.3135\n",
      "Epoch: 0, batch: 64 Loss: 7.9159\n",
      "Epoch: 0, batch: 65 Loss: 0.3579\n",
      "Epoch: 0, batch: 66 Loss: 1.2755\n",
      "Epoch: 0, batch: 67 Loss: 0.4631\n",
      "Epoch: 0, batch: 68 Loss: 29.9136\n",
      "Epoch: 0, batch: 69 Loss: 0.5211\n",
      "Epoch: 0, batch: 70 Loss: 0.4683\n",
      "Epoch: 0, batch: 71 Loss: 12.6452\n",
      "Epoch: 0, batch: 72 Loss: 0.5164\n",
      "Epoch: 0, batch: 73 Loss: 2.2403\n",
      "Epoch: 0, batch: 74 Loss: 0.6697\n",
      "Epoch: 0, batch: 75 Loss: 0.5684\n",
      "Epoch: 0, batch: 76 Loss: 0.5596\n",
      "Epoch: 0, batch: 77 Loss: 0.7741\n",
      "Epoch: 0, batch: 78 Loss: 14.6714\n",
      "Epoch: 0, batch: 79 Loss: 3.5328\n",
      "Epoch: 0, batch: 80 Loss: 2.3823\n",
      "Epoch: 0, batch: 81 Loss: 0.4251\n",
      "Epoch: 0, batch: 82 Loss: 5.0475\n",
      "Epoch: 0, batch: 83 Loss: 0.5196\n",
      "Epoch: 0, batch: 84 Loss: 0.5121\n",
      "Epoch: 0, batch: 85 Loss: 0.6527\n",
      "Epoch: 0, batch: 86 Loss: 2.2289\n",
      "Epoch: 0, batch: 87 Loss: 3.4822\n",
      "Epoch: 0, batch: 88 Loss: 6.0416\n",
      "Epoch: 0, batch: 89 Loss: 2.0765\n",
      "Epoch: 0, batch: 90 Loss: 2.4073\n",
      "Epoch: 0, batch: 91 Loss: 0.5418\n",
      "Epoch: 0, batch: 92 Loss: 1.2269\n",
      "Epoch: 0, batch: 93 Loss: 0.5361\n",
      "Epoch: 0, batch: 94 Loss: 3.3744\n",
      "Epoch: 0, batch: 95 Loss: 0.5260\n",
      "Epoch: 0, batch: 96 Loss: 2.2250\n",
      "Epoch: 0, batch: 97 Loss: 0.5098\n",
      "Epoch: 0, batch: 98 Loss: 0.6914\n",
      "Epoch: 0, batch: 99 Loss: 9.5156\n",
      "Epoch: 0, batch: 100 Loss: 14.8554\n",
      "Epoch: 0, batch: 101 Loss: 0.4768\n",
      "Epoch: 0, batch: 102 Loss: 1.2860\n",
      "Epoch: 0, batch: 103 Loss: 3.2443\n",
      "Epoch: 0, batch: 104 Loss: 3.4919\n",
      "Epoch: 0, batch: 105 Loss: 0.6233\n",
      "Epoch: 0, batch: 106 Loss: 0.4959\n",
      "Epoch: 0, batch: 107 Loss: 2.1566\n",
      "Epoch: 0, batch: 108 Loss: 0.6227\n",
      "Epoch: 0, batch: 109 Loss: 3.4915\n",
      "Epoch: 0, batch: 110 Loss: 3.9026\n",
      "Epoch: 0, batch: 111 Loss: 0.4882\n",
      "Epoch: 0, batch: 112 Loss: 5.2274\n",
      "Epoch: 0, batch: 113 Loss: 0.4967\n",
      "Epoch: 0, batch: 114 Loss: 0.4944\n",
      "Epoch: 0, batch: 115 Loss: 1.2880\n",
      "Epoch: 0, batch: 116 Loss: 0.4701\n",
      "Epoch: 0, batch: 117 Loss: 0.4497\n",
      "Epoch: 0, batch: 118 Loss: 0.3729\n",
      "Epoch: 0, batch: 119 Loss: 5.3859\n",
      "Epoch: 0, batch: 120 Loss: 2.1477\n",
      "Epoch: 0, batch: 121 Loss: 0.3684\n",
      "Epoch: 0, batch: 122 Loss: 1.2589\n",
      "Epoch: 0, batch: 123 Loss: 2.2292\n",
      "Epoch: 0, batch: 124 Loss: 15.0842\n",
      "Epoch: 0, batch: 125 Loss: 2.5158\n",
      "Epoch: 0, batch: 126 Loss: 1.2306\n",
      "Epoch: 0, batch: 127 Loss: 0.6154\n",
      "Epoch: 0, batch: 128 Loss: 16.9654\n",
      "Epoch: 0, batch: 129 Loss: 0.5920\n",
      "Epoch: 0, batch: 130 Loss: 25.0664\n",
      "Epoch: 0, batch: 131 Loss: 2.0871\n",
      "Epoch: 0, batch: 132 Loss: 0.4483\n",
      "Epoch: 0, batch: 133 Loss: 0.4830\n",
      "Epoch: 0, batch: 134 Loss: 3.6097\n",
      "Epoch: 0, batch: 135 Loss: 0.3812\n",
      "Epoch: 0, batch: 136 Loss: 0.8130\n",
      "Epoch: 0, batch: 137 Loss: 5.2155\n",
      "Epoch: 0, batch: 138 Loss: 4.1917\n",
      "Epoch: 0, batch: 139 Loss: 11.3179\n",
      "Epoch: 0, batch: 140 Loss: 0.6070\n",
      "Epoch: 0, batch: 141 Loss: 0.6011\n",
      "Epoch: 0, batch: 142 Loss: 0.8178\n",
      "Epoch: 0, batch: 143 Loss: 2.1451\n",
      "Epoch: 0, batch: 144 Loss: 9.6455\n",
      "Epoch: 0, batch: 145 Loss: 7.6510\n",
      "Epoch: 0, batch: 146 Loss: 3.5248\n",
      "Epoch: 0, batch: 147 Loss: 0.8552\n",
      "Epoch: 0, batch: 148 Loss: 1.0755\n",
      "Epoch: 0, batch: 149 Loss: 8.7695\n",
      "Epoch: 0, batch: 150 Loss: 5.0084\n",
      "Epoch: 0, batch: 151 Loss: 2.1420\n",
      "Epoch: 0, batch: 152 Loss: 2.8070\n",
      "Epoch: 0, batch: 153 Loss: 1.0010\n",
      "Epoch: 0, batch: 154 Loss: 3.5710\n",
      "Epoch: 0, batch: 155 Loss: 1.3040\n",
      "Epoch: 0, batch: 156 Loss: 3.3455\n",
      "Epoch: 0, batch: 157 Loss: 8.9530\n",
      "Epoch: 0, batch: 158 Loss: 1.5629\n",
      "Epoch: 0, batch: 159 Loss: 2.8245\n",
      "Epoch: 0, batch: 160 Loss: 7.9828\n",
      "Epoch: 0, batch: 161 Loss: 1.5090\n",
      "Epoch: 0, batch: 162 Loss: 0.8375\n",
      "Epoch: 0, batch: 163 Loss: 0.8056\n",
      "Epoch: 0, batch: 164 Loss: 8.2505\n",
      "Epoch: 0, batch: 165 Loss: 0.9619\n",
      "Epoch: 0, batch: 166 Loss: 0.8466\n",
      "Epoch: 0, batch: 167 Loss: 0.7973\n",
      "Epoch: 0, batch: 168 Loss: 2.0212\n",
      "Epoch: 0, batch: 169 Loss: 0.6781\n",
      "Epoch: 0, batch: 170 Loss: 1.3320\n",
      "Epoch: 0, batch: 171 Loss: 3.3953\n",
      "Epoch: 0, batch: 172 Loss: 12.4065\n",
      "Epoch: 0, batch: 173 Loss: 0.5137\n",
      "Epoch: 0, batch: 174 Loss: 0.6926\n",
      "Epoch: 0, batch: 175 Loss: 5.2411\n",
      "Epoch: 0, batch: 176 Loss: 0.6779\n",
      "Epoch: 0, batch: 177 Loss: 0.3973\n",
      "Epoch: 0, batch: 178 Loss: 0.3856\n",
      "Epoch: 0, batch: 179 Loss: 9.3579\n",
      "Epoch: 0, batch: 180 Loss: 0.6505\n",
      "Epoch: 0, batch: 181 Loss: 4.3804\n",
      "Epoch: 0, batch: 182 Loss: 0.3532\n",
      "Epoch: 0, batch: 183 Loss: 0.6401\n",
      "Epoch: 0, batch: 184 Loss: 0.3801\n",
      "Epoch: 0, batch: 185 Loss: 0.6297\n",
      "Epoch: 0, batch: 186 Loss: 0.3522\n",
      "Epoch: 0, batch: 187 Loss: 0.3357\n",
      "Epoch: 0, batch: 188 Loss: 3.8366\n",
      "Epoch: 0, batch: 189 Loss: 2.2003\n",
      "Epoch: 0, batch: 190 Loss: 0.6005\n",
      "Epoch: 0, batch: 191 Loss: 1.2515\n",
      "Epoch: 0, batch: 192 Loss: 10.8014\n",
      "Epoch: 0, batch: 193 Loss: 0.2751\n",
      "Epoch: 0, batch: 194 Loss: 8.4723\n",
      "Epoch: 0, batch: 195 Loss: 4.1579\n",
      "Epoch: 0, batch: 196 Loss: 1.2538\n",
      "Epoch: 0, batch: 197 Loss: 0.5890\n",
      "Epoch: 0, batch: 198 Loss: 20.1008\n",
      "Epoch: 0, batch: 199 Loss: 0.3789\n",
      "Epoch: 0, batch: 200 Loss: 0.6025\n",
      "Epoch: 0, batch: 201 Loss: 0.4332\n",
      "Epoch: 0, batch: 202 Loss: 0.4505\n",
      "Epoch: 0, batch: 203 Loss: 11.9266\n",
      "Epoch: 0, batch: 204 Loss: 0.4128\n",
      "Epoch: 0, batch: 205 Loss: 4.2934\n",
      "Epoch: 0, batch: 206 Loss: 3.4181\n",
      "Epoch: 0, batch: 207 Loss: 1.2997\n",
      "Epoch: 0, batch: 208 Loss: 0.6375\n",
      "Epoch: 0, batch: 209 Loss: 0.5481\n",
      "Epoch: 0, batch: 210 Loss: 0.4668\n",
      "Epoch: 0, batch: 211 Loss: 0.7152\n",
      "Epoch: 0, batch: 212 Loss: 0.5229\n",
      "Epoch: 0, batch: 213 Loss: 0.5009\n",
      "Epoch: 0, batch: 214 Loss: 0.4727\n",
      "Epoch: 0, batch: 215 Loss: 3.0578\n",
      "Epoch: 0, batch: 216 Loss: 2.4572\n",
      "Epoch: 0, batch: 217 Loss: 1.5111\n",
      "Epoch: 0, batch: 218 Loss: 0.3510\n",
      "Epoch: 0, batch: 219 Loss: 7.0700\n",
      "Epoch: 0, batch: 220 Loss: 1.5215\n",
      "Epoch: 0, batch: 221 Loss: 3.5228\n",
      "Epoch: 0, batch: 222 Loss: 0.5591\n",
      "Epoch: 0, batch: 223 Loss: 0.8925\n",
      "Epoch: 0, batch: 224 Loss: 2.1902\n",
      "Epoch: 0, batch: 225 Loss: 0.6313\n",
      "Epoch: 0, batch: 226 Loss: 2.1897\n",
      "Epoch: 0, batch: 227 Loss: 0.3709\n",
      "Epoch: 0, batch: 228 Loss: 0.6297\n",
      "Epoch: 0, batch: 229 Loss: 0.3612\n",
      "Epoch: 0, batch: 230 Loss: 2.4373\n",
      "Epoch: 0, batch: 231 Loss: 0.3483\n",
      "Epoch: 0, batch: 232 Loss: 3.5063\n",
      "Epoch: 0, batch: 233 Loss: 0.3108\n",
      "Epoch: 0, batch: 234 Loss: 5.1539\n",
      "Epoch: 0, batch: 235 Loss: 1.1802\n",
      "Epoch: 0, batch: 236 Loss: 1.2302\n",
      "Epoch: 0, batch: 237 Loss: 0.3356\n",
      "Epoch: 0, batch: 238 Loss: 0.3336\n",
      "Epoch: 0, batch: 239 Loss: 0.8735\n",
      "Epoch: 0, batch: 240 Loss: 6.0918\n",
      "Epoch: 0, batch: 241 Loss: 1.2555\n",
      "Epoch: 0, batch: 242 Loss: 7.4348\n",
      "Epoch: 0, batch: 243 Loss: 0.3146\n",
      "Epoch: 0, batch: 244 Loss: 3.5003\n",
      "Epoch: 0, batch: 245 Loss: 5.1280\n",
      "Epoch: 0, batch: 246 Loss: 12.1571\n",
      "Epoch: 0, batch: 247 Loss: 7.0463\n",
      "Epoch: 0, batch: 248 Loss: 1.2223\n",
      "Epoch: 0, batch: 249 Loss: 44.6940\n",
      "Epoch: 0, batch: 250 Loss: 0.4548\n",
      "Epoch: 0, batch: 251 Loss: 0.7415\n",
      "Epoch: 0, batch: 252 Loss: 6.9446\n",
      "Epoch: 0, batch: 253 Loss: 3.9306\n",
      "Epoch: 0, batch: 254 Loss: 0.7459\n",
      "Epoch: 0, batch: 255 Loss: 1.4009\n",
      "Epoch: 0, batch: 256 Loss: 4.8733\n",
      "Epoch: 0, batch: 257 Loss: 1.5301\n",
      "Epoch: 0, batch: 258 Loss: 3.4853\n",
      "Epoch: 0, batch: 259 Loss: 11.5352\n",
      "Epoch: 0, batch: 260 Loss: 0.9810\n",
      "Epoch: 0, batch: 261 Loss: 0.8292\n",
      "Epoch: 0, batch: 262 Loss: 0.8174\n",
      "Epoch: 0, batch: 263 Loss: 1.8437\n",
      "Epoch: 0, batch: 264 Loss: 2.2110\n",
      "Epoch: 0, batch: 265 Loss: 1.4552\n",
      "Epoch: 0, batch: 266 Loss: 13.0108\n",
      "Epoch: 0, batch: 267 Loss: 0.6093\n",
      "Epoch: 0, batch: 268 Loss: 4.8778\n",
      "Epoch: 0, batch: 269 Loss: 0.9583\n",
      "Epoch: 0, batch: 270 Loss: 0.7811\n",
      "Epoch: 0, batch: 271 Loss: 5.0020\n",
      "Epoch: 0, batch: 272 Loss: 1.4487\n",
      "Epoch: 0, batch: 273 Loss: 0.7212\n",
      "Epoch: 0, batch: 274 Loss: 0.8208\n",
      "Epoch: 0, batch: 275 Loss: 1.3895\n",
      "Epoch: 0, batch: 276 Loss: 0.7045\n",
      "Epoch: 0, batch: 277 Loss: 0.6421\n",
      "Epoch: 0, batch: 278 Loss: 0.6509\n",
      "Epoch: 0, batch: 279 Loss: 0.5220\n",
      "Epoch: 0, batch: 280 Loss: 0.4674\n",
      "Epoch: 0, batch: 281 Loss: 2.1741\n",
      "Epoch: 0, batch: 282 Loss: 0.3773\n",
      "Epoch: 0, batch: 283 Loss: 0.6183\n",
      "Epoch: 0, batch: 284 Loss: 0.3097\n",
      "Epoch: 0, batch: 285 Loss: 0.2710\n",
      "Epoch: 0, batch: 286 Loss: 1.2500\n",
      "Epoch: 0, batch: 287 Loss: 4.6123\n",
      "Epoch: 0, batch: 288 Loss: 22.0388\n",
      "Epoch: 0, batch: 289 Loss: 3.6162\n",
      "Epoch: 0, batch: 290 Loss: 0.2177\n",
      "Epoch: 0, batch: 291 Loss: 0.2392\n",
      "Epoch: 0, batch: 292 Loss: 3.6111\n",
      "Epoch: 0, batch: 293 Loss: 4.6528\n",
      "Epoch: 0, batch: 294 Loss: 5.6628\n",
      "Epoch: 0, batch: 295 Loss: 0.5845\n",
      "Epoch: 0, batch: 296 Loss: 2.2606\n",
      "Epoch: 0, batch: 297 Loss: 3.6080\n",
      "Epoch: 0, batch: 298 Loss: 2.2562\n",
      "Epoch: 0, batch: 299 Loss: 0.5798\n",
      "Epoch: 0, batch: 300 Loss: 0.5815\n",
      "Epoch: 0, batch: 301 Loss: 0.2486\n",
      "Epoch: 0, batch: 302 Loss: 0.2505\n",
      "Epoch: 0, batch: 303 Loss: 0.2502\n",
      "Epoch: 0, batch: 304 Loss: 0.2488\n",
      "Epoch: 0, batch: 305 Loss: 2.2515\n",
      "Epoch: 0, batch: 306 Loss: 40.2905\n",
      "Epoch: 0, batch: 307 Loss: 0.2517\n",
      "Epoch: 0, batch: 308 Loss: 12.2305\n",
      "Epoch: 0, batch: 309 Loss: 12.2096\n",
      "Epoch: 0, batch: 310 Loss: 0.2740\n",
      "Epoch: 0, batch: 311 Loss: 0.8885\n",
      "Epoch: 0, batch: 312 Loss: 15.1015\n",
      "Epoch: 0, batch: 313 Loss: 4.1083\n",
      "Epoch: 0, batch: 314 Loss: 0.3524\n",
      "Epoch: 0, batch: 315 Loss: 0.5589\n",
      "Epoch: 0, batch: 316 Loss: 2.1829\n",
      "Epoch: 0, batch: 317 Loss: 2.1309\n",
      "Epoch: 0, batch: 318 Loss: 0.3249\n",
      "Epoch: 0, batch: 319 Loss: 0.3882\n",
      "Epoch: 0, batch: 320 Loss: 0.4542\n",
      "Epoch: 0, batch: 321 Loss: 7.0458\n",
      "Epoch: 0, batch: 322 Loss: 3.4950\n",
      "Epoch: 0, batch: 323 Loss: 0.6817\n",
      "Epoch: 0, batch: 324 Loss: 0.6201\n",
      "Epoch: 0, batch: 325 Loss: 7.2341\n",
      "Epoch: 0, batch: 326 Loss: 0.4237\n",
      "Epoch: 0, batch: 327 Loss: 0.4941\n",
      "Epoch: 0, batch: 328 Loss: 7.7560\n",
      "Epoch: 0, batch: 329 Loss: 0.4296\n",
      "Epoch: 0, batch: 330 Loss: 2.2239\n",
      "Epoch: 0, batch: 331 Loss: 16.4617\n",
      "Epoch: 0, batch: 332 Loss: 3.4847\n",
      "Epoch: 0, batch: 333 Loss: 9.2469\n",
      "Epoch: 0, batch: 334 Loss: 0.4813\n",
      "Epoch: 0, batch: 335 Loss: 2.2314\n",
      "Epoch: 0, batch: 336 Loss: 3.4468\n",
      "Epoch: 0, batch: 337 Loss: 0.6180\n",
      "Epoch: 0, batch: 338 Loss: 0.6278\n",
      "Epoch: 0, batch: 339 Loss: 0.6259\n",
      "Epoch: 0, batch: 340 Loss: 3.0028\n",
      "Epoch: 0, batch: 341 Loss: 0.6637\n",
      "Epoch: 0, batch: 342 Loss: 0.6101\n",
      "Epoch: 0, batch: 343 Loss: 2.2335\n",
      "Epoch: 0, batch: 344 Loss: 0.7413\n",
      "Epoch: 0, batch: 345 Loss: 1.3141\n",
      "Epoch: 0, batch: 346 Loss: 3.4002\n",
      "Epoch: 0, batch: 347 Loss: 0.7151\n",
      "Epoch: 0, batch: 348 Loss: 0.5215\n",
      "Epoch: 0, batch: 349 Loss: 0.4314\n",
      "Epoch: 0, batch: 350 Loss: 6.8413\n",
      "Epoch: 0, batch: 351 Loss: 0.4658\n",
      "Epoch: 0, batch: 352 Loss: 5.2709\n",
      "Epoch: 0, batch: 353 Loss: 0.6660\n",
      "Epoch: 0, batch: 354 Loss: 0.3812\n",
      "Epoch: 0, batch: 355 Loss: 0.3724\n",
      "Epoch: 0, batch: 356 Loss: 22.0930\n",
      "Epoch: 0, batch: 357 Loss: 9.5648\n",
      "Epoch: 0, batch: 358 Loss: 0.3691\n",
      "Epoch: 0, batch: 359 Loss: 2.1224\n",
      "Epoch: 0, batch: 360 Loss: 2.0147\n",
      "Epoch: 0, batch: 361 Loss: 0.4420\n",
      "Epoch: 0, batch: 362 Loss: 0.6681\n",
      "Epoch: 0, batch: 363 Loss: 0.3927\n",
      "Epoch: 0, batch: 364 Loss: 3.4431\n",
      "Epoch: 0, batch: 365 Loss: 2.9965\n",
      "Epoch: 0, batch: 366 Loss: 0.6136\n",
      "Epoch: 0, batch: 367 Loss: 0.4584\n",
      "Epoch: 0, batch: 368 Loss: 0.6726\n",
      "Epoch: 0, batch: 369 Loss: 7.0504\n",
      "Epoch: 0, batch: 370 Loss: 0.3940\n",
      "Epoch: 0, batch: 371 Loss: 3.4432\n",
      "Epoch: 0, batch: 372 Loss: 0.4457\n",
      "Epoch: 0, batch: 373 Loss: 0.4390\n",
      "Epoch: 0, batch: 374 Loss: 0.4277\n",
      "Epoch: 0, batch: 375 Loss: 1.7471\n",
      "Epoch: 0, batch: 376 Loss: 0.4034\n",
      "Epoch: 0, batch: 377 Loss: 0.3490\n",
      "Epoch: 0, batch: 378 Loss: 2.1871\n",
      "Epoch: 0, batch: 379 Loss: 3.1214\n",
      "Epoch: 0, batch: 380 Loss: 0.3596\n",
      "Epoch: 0, batch: 381 Loss: 0.3506\n",
      "Epoch: 0, batch: 382 Loss: 9.3979\n",
      "Epoch: 0, batch: 383 Loss: 0.3353\n",
      "Epoch: 0, batch: 384 Loss: 1.2309\n",
      "Epoch: 0, batch: 385 Loss: 0.3233\n",
      "Epoch: 0, batch: 386 Loss: 2.2332\n",
      "Epoch: 0, batch: 387 Loss: 7.1035\n",
      "Epoch: 0, batch: 388 Loss: 0.2722\n",
      "Epoch: 0, batch: 389 Loss: 5.1967\n",
      "Epoch: 0, batch: 390 Loss: 0.3115\n",
      "Epoch: 0, batch: 391 Loss: 0.3104\n",
      "Epoch: 0, batch: 392 Loss: 1.5504\n",
      "Epoch: 0, batch: 393 Loss: 1.5341\n",
      "Epoch: 0, batch: 394 Loss: 3.5512\n",
      "Epoch: 0, batch: 395 Loss: 5.4786\n",
      "Epoch: 0, batch: 396 Loss: 14.0552\n",
      "Epoch: 0, batch: 397 Loss: 6.7537\n",
      "Epoch: 0, batch: 398 Loss: 0.8949\n",
      "Epoch: 0, batch: 399 Loss: 3.5244\n",
      "Epoch: 0, batch: 400 Loss: 5.3564\n",
      "Epoch: 0, batch: 401 Loss: 3.5095\n",
      "Epoch: 0, batch: 402 Loss: 1.5024\n",
      "Epoch: 0, batch: 403 Loss: 0.3414\n",
      "Epoch: 0, batch: 404 Loss: 1.2230\n",
      "Epoch: 0, batch: 405 Loss: 0.5004\n",
      "Epoch: 0, batch: 406 Loss: 5.8662\n",
      "Epoch: 0, batch: 407 Loss: 0.5292\n",
      "Epoch: 0, batch: 408 Loss: 3.6599\n",
      "Epoch: 0, batch: 409 Loss: 2.2279\n",
      "Epoch: 0, batch: 410 Loss: 6.9796\n",
      "Epoch: 0, batch: 411 Loss: 1.3048\n",
      "Epoch: 0, batch: 412 Loss: 0.5046\n",
      "Epoch: 0, batch: 413 Loss: 0.6050\n",
      "Epoch: 0, batch: 414 Loss: 0.5677\n",
      "Epoch: 0, batch: 415 Loss: 0.6013\n",
      "Epoch: 0, batch: 416 Loss: 0.7442\n",
      "Epoch: 0, batch: 417 Loss: 1.3161\n",
      "Epoch: 0, batch: 418 Loss: 0.6437\n",
      "Epoch: 0, batch: 419 Loss: 0.3827\n",
      "Epoch: 0, batch: 420 Loss: 1.4115\n",
      "Epoch: 0, batch: 421 Loss: 0.5047\n",
      "Epoch: 0, batch: 422 Loss: 0.4855\n",
      "Epoch: 0, batch: 423 Loss: 9.2557\n",
      "Epoch: 0, batch: 424 Loss: 0.6692\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-8ce994e5cea5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m50\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[1;31m#print(\"Epoch: {}, batch: {} Loss: {} label_loss:{}\".format(i, batch_idx, loss, label_loss_))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Epoch: {}, batch: {} Loss: {:0.4f}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# begin testing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36m__format__\u001b[1;34m(self, format_spec)\u001b[0m\n\u001b[0;32m    558\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__format__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat_spec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__format__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformat_spec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    561\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__format__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat_spec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training\n",
    "loss_list = []\n",
    "alpha=0.2\n",
    "epochs = 100\n",
    "\n",
    "\n",
    "accuracy_list = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    net.train()\n",
    "    for (batch_idx, batch) in enumerate(trainloader):\n",
    "        XZ_train_batch = batch[0].cuda() # remove .cuda() if you don't have a GPU\n",
    "        YZ_train_batch = batch[1].cuda() # remove .cuda() if you don't have a GPU\n",
    "        sig_train_batch = batch[2].cuda() # remove .cuda() if you don't have a GPU\n",
    "\n",
    "        Netout = net.forward(XZ_train_batch, YZ_train_batch) # This will call the forward function, usually it returns tensors.\n",
    "        #print(F.softmax(Netout))\n",
    "        loss = criterion(Netout, sig_train_batch) # classification loss\n",
    "\n",
    "        # Zero the gradients before running the backward pass.\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "        # parameters of the model. Internally, the parameters of each Module are stored\n",
    "        # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "        # all learnable parameters in the model.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Calling the step function on an Optimizer makes an update to its\n",
    "        # parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_list.append(loss)\n",
    "        if batch_idx % 50 == 0 or True:\n",
    "            #print(\"Epoch: {}, batch: {} Loss: {} label_loss:{}\".format(i, batch_idx, loss, label_loss_))\n",
    "            print(\"Epoch: {}, batch: {} Loss: {:0.4f}\".format(i, batch_idx, loss))\n",
    "    \n",
    "    net.eval() # begin testing\n",
    "    preds = np.array([])\n",
    "    reals = np.array([])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (batch_idx, batch) in enumerate(testloader):\n",
    "            XZ_test_batch = batch[0].cuda() # remove .cuda() if you don't have a GPU\n",
    "            YZ_test_batch = batch[1].cuda() # remove .cuda() if you don't have a GPU\n",
    "            sig_test_batch = batch[2].cuda() # remove .cuda() if you don't have a GPU\n",
    "\n",
    "            Netout = net.forward(XZ_test_batch, YZ_test_batch) # This will call the forward function, usually it returns tensors.\n",
    "            #print(Netout.shape)\n",
    "            prediction=F.softmax(Netout, dim=1).argmax(dim=1)\n",
    "            \n",
    "\n",
    "            preds=np.concatenate((preds, prediction.cpu().detach().numpy().flatten()))\n",
    "            reals=np.concatenate((reals, sig_test_batch.cpu().detach().numpy().flatten()))\n",
    "        preds=np.array(preds)\n",
    "        reals=np.array(reals)\n",
    "        accuracy=np.mean(preds==reals)\n",
    "        accuracy_list.append(accuracy)\n",
    "        print(\"Test accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siglist=[]\n",
    "for items in file_list:\n",
    "    data = np.load( os.path.join( data_path, items ) )\n",
    "    siglist.append([items, data['sig']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siglist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "preds = []\n",
    "reals = []\n",
    "\n",
    "for (batch_idx, batch) in enumerate(testloader):\n",
    "    XZ_test_batch = batch[0].cuda() # remove .cuda() if you don't have a GPU\n",
    "    YZ_test_batch = batch[1].cuda() # remove .cuda() if you don't have a GPU\n",
    "    sig_test_batch = batch[2].cuda() # remove .cuda() if you don't have a GPU\n",
    "\n",
    "    Netout = net.forward(XZ_test_batch, YZ_test_batch) # This will call the forward function, usually it returns tensors.\n",
    "    #print(Netout.shape)\n",
    "    prediction=Netout\n",
    "    #print(predictor)\n",
    "\n",
    "    preds.append(prediction.cpu().detach().numpy())\n",
    "    \n",
    "    reals.append(sig_test_batch.cpu().detach().numpy())\n",
    "preds=np.array(preds)\n",
    "reals=np.array(reals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_cm = pd.DataFrame(confusion_matrix(reals.flatten(), preds.flatten()), index = ['background', 'signal'],\n",
    "                  columns = ['background', 'signal'])\n",
    "plt.figure(figsize = (10,7))\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "sn.heatmap(df_cm, annot=True, cmap=\"jet\")\n",
    "plt.show()\n",
    "\n",
    "# Confusion matrix whose i-th row and j-th column entry indicates the number of samples with true label being i-th class and predicted label being j-th class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(reals.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net, './model_save_train_VGG_reduced_time.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.load('./model_save_train_VGG_reduced_time.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 45,  12],\n",
       "       [  8, 335]], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(reals.flatten(), preds.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.40362734],\n",
       "        [0.40550905]],\n",
       "\n",
       "       [[0.22553435],\n",
       "        [0.40490067]],\n",
       "\n",
       "       [[0.39987746],\n",
       "        [0.40550905]],\n",
       "\n",
       "       [[0.39987746],\n",
       "        [0.40550905]],\n",
       "\n",
       "       [[0.39987746],\n",
       "        [0.40613887]],\n",
       "\n",
       "       [[0.4067989 ],\n",
       "        [0.40631136]],\n",
       "\n",
       "       [[0.22553435],\n",
       "        [0.40345606]],\n",
       "\n",
       "       [[0.4066822 ],\n",
       "        [0.4066822 ]],\n",
       "\n",
       "       [[0.22553435],\n",
       "        [0.40550905]],\n",
       "\n",
       "       [[0.40631136],\n",
       "        [0.40550905]],\n",
       "\n",
       "       [[0.40362734],\n",
       "        [0.4067989 ]],\n",
       "\n",
       "       [[0.40631136],\n",
       "        [0.40550905]],\n",
       "\n",
       "       [[0.40550905],\n",
       "        [0.22553435]],\n",
       "\n",
       "       [[0.40345606],\n",
       "        [0.40631136]],\n",
       "\n",
       "       [[0.40550905],\n",
       "        [0.40613887]],\n",
       "\n",
       "       [[0.40490067],\n",
       "        [0.40490067]],\n",
       "\n",
       "       [[0.4067989 ],\n",
       "        [0.4067989 ]],\n",
       "\n",
       "       [[0.40490067],\n",
       "        [0.40550905]],\n",
       "\n",
       "       [[0.40345606],\n",
       "        [0.40490067]],\n",
       "\n",
       "       [[0.40345606],\n",
       "        [0.4067989 ]],\n",
       "\n",
       "       [[0.40631136],\n",
       "        [0.39987746]],\n",
       "\n",
       "       [[0.40631136],\n",
       "        [0.40345606]],\n",
       "\n",
       "       [[0.40613887],\n",
       "        [0.40613887]],\n",
       "\n",
       "       [[0.40613887],\n",
       "        [0.40490067]],\n",
       "\n",
       "       [[0.4066822 ],\n",
       "        [0.4067989 ]],\n",
       "\n",
       "       [[0.40490067],\n",
       "        [0.40613887]],\n",
       "\n",
       "       [[0.40345606],\n",
       "        [0.40550905]],\n",
       "\n",
       "       [[0.40362734],\n",
       "        [0.39987746]],\n",
       "\n",
       "       [[0.39987746],\n",
       "        [0.4066822 ]],\n",
       "\n",
       "       [[0.40345606],\n",
       "        [0.4067989 ]],\n",
       "\n",
       "       [[0.4066822 ],\n",
       "        [0.4066822 ]],\n",
       "\n",
       "       [[0.40550905],\n",
       "        [0.4066822 ]],\n",
       "\n",
       "       [[0.40613887],\n",
       "        [0.39987746]],\n",
       "\n",
       "       [[0.40362734],\n",
       "        [0.22553435]],\n",
       "\n",
       "       [[0.4066822 ],\n",
       "        [0.40362734]],\n",
       "\n",
       "       [[0.40490067],\n",
       "        [0.40345606]],\n",
       "\n",
       "       [[0.40362734],\n",
       "        [0.4067989 ]],\n",
       "\n",
       "       [[0.40345606],\n",
       "        [0.39987746]],\n",
       "\n",
       "       [[0.4067989 ],\n",
       "        [0.40613887]],\n",
       "\n",
       "       [[0.4067989 ],\n",
       "        [0.40550905]]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6e343a66b0d3f3fb9e5b3006acd45e89d57a985b4e0912ddff9600a29bb2e852"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
