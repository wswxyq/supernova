{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from os import listdir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = ['fd-1kpc-9.6sm-0-overlaid.csv/npz_subevents']\n",
    "file_list = []\n",
    "for path_ in data_path:\n",
    "    for file_ in listdir(path_):\n",
    "        file_list.append(os.path.join(path_, file_))\n",
    "file_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"siglist=[]\\nfor items in file_list:\\n    data = np.load( items )\\n    siglist.append(data['sig'])\\n\\nsigindex=np.array(file_list, dtype='U')[np.array(siglist, dtype=int)==1]\\nbkgindex=np.array(file_list, dtype='U')[np.array(siglist, dtype=int)==0]\\nprint(len(sigindex))\\nprint(len(bkgindex))\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''siglist=[]\n",
    "for items in file_list:\n",
    "    data = np.load( items )\n",
    "    siglist.append(data['sig'])\n",
    "\n",
    "sigindex=np.array(file_list, dtype='U')[np.array(siglist, dtype=int)==1]\n",
    "bkgindex=np.array(file_list, dtype='U')[np.array(siglist, dtype=int)==0]\n",
    "print(len(sigindex))\n",
    "print(len(bkgindex))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image size\n",
    "z_size = 448\n",
    "xy_size = 384\n",
    "\n",
    "\n",
    "def shadow(pixel_loc, matrix, step=1):\n",
    "    for i in range( np.max([pixel_loc[0]-step, 0]), np.min([pixel_loc[0]+step+1, matrix.shape[0]]) ):\n",
    "        for j in range( np.max([pixel_loc[1]-step, 0]), np.min([pixel_loc[1]+step+1, matrix.shape[1]])):\n",
    "            matrix[i, j] = 1\n",
    "    # in place modification of the matrix\n",
    "\n",
    "shadowstep=10\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, file_list_):\n",
    "        self.file_list = file_list_\n",
    "        self.len = len(file_list_)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        file_name = self.file_list[index]\n",
    "        data = np.load( file_name ) \n",
    "        labelxz = data['labelxz']\n",
    "        labelyz = data['labelyz']\n",
    "        labelxzimg=np.zeros((z_size, xy_size), dtype=int)\n",
    "        labelyzimg=np.zeros((z_size, xy_size), dtype=int)\n",
    "        for i in range(labelxz.shape[0]):\n",
    "            shadow(labelxz[i,:], labelxzimg, step=shadowstep)\n",
    "        for i in range(labelyz.shape[0]):\n",
    "            shadow(labelyz[i,:], labelyzimg, step=shadowstep)\n",
    "\n",
    "        return torch.from_numpy(data['imxz'][:, :, :]).to(torch.float), \\\n",
    "                torch.from_numpy(data['imyz'][:, :, :]).to(torch.float), \\\n",
    "                torch.from_numpy(labelxzimg).to(torch.long), \\\n",
    "                torch.from_numpy(labelyzimg).to(torch.long)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "mydataset = MyDataset(file_list)\n",
    "batch_size_train = 4\n",
    "batch_size_test = 2\n",
    "\n",
    "\n",
    "train_size = int(0.8 * len(mydataset))\n",
    "test_size = len(mydataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(mydataset, [train_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size_train,\n",
    "                                            shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size_test,\n",
    "                                            shuffle=False)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pool = nn.MaxPool2d(2, 2, ceil_mode=True)\n",
    "        self.unpool=nn.Upsample(scale_factor=2, mode='nearest')\n",
    "\n",
    "        self.conv1_1 = nn.Conv2d(2, 64, 3) \n",
    "        self.conv1_2 = nn.Conv2d(2, 64, 3) \n",
    "        \n",
    "        self.conv2_1 = nn.Conv2d(64, 128, 3)\n",
    "        self.conv2_2 = nn.Conv2d(64, 128, 3)\n",
    "\n",
    "        self.conv3_1 = nn.Conv2d(128, 256, 3)\n",
    "        self.conv3_2 = nn.Conv2d(128, 256, 3) \n",
    "\n",
    "        self.conv4_1 = nn.Conv2d(256, 256, 3)\n",
    "        self.conv4_2 = nn.Conv2d(256, 256, 3)\n",
    "\n",
    "        self.conv5_1 = nn.Conv2d(256, 512, 3)\n",
    "        self.conv5_2 = nn.Conv2d(256, 512, 3)\n",
    "\n",
    "        self.conv6_1 = nn.Conv2d(512, 512, 3)\n",
    "        self.conv6_2 = nn.Conv2d(512, 512, 3)\n",
    "\n",
    "        self.conv7_1 = nn.Conv2d(512, 512, 3)\n",
    "        self.conv7_2 = nn.Conv2d(512, 512, 3)\n",
    "\n",
    "        self.conv8_1 = nn.Conv2d(512, 512, 3)\n",
    "        self.conv8_2 = nn.Conv2d(512, 512, 3)\n",
    "\n",
    "        self.convtrans1_1 = nn.ConvTranspose2d(1024, 512, 2)\n",
    "        self.convtrans1_2 = nn.ConvTranspose2d(1024, 512, 2)\n",
    "\n",
    "        self.convtrans2_1 = nn.ConvTranspose2d(512, 512, 2)\n",
    "        self.convtrans2_2 = nn.ConvTranspose2d(512, 512, 2)\n",
    "\n",
    "        self.convtrans3_1 = nn.ConvTranspose2d(512, 256, 2)\n",
    "        self.convtrans3_2 = nn.ConvTranspose2d(512, 256, 2)\n",
    "\n",
    "        self.convtrans4_1 = nn.ConvTranspose2d(256, 256, 2)\n",
    "        self.convtrans4_2 = nn.ConvTranspose2d(256, 256, 2)\n",
    "\n",
    "        self.convtrans5_1 = nn.ConvTranspose2d(256, 128, 2)\n",
    "        self.convtrans5_2 = nn.ConvTranspose2d(256, 128, 2)\n",
    "\n",
    "        self.convtrans6_1 = nn.ConvTranspose2d(128, 64, 3)\n",
    "        self.convtrans6_2 = nn.ConvTranspose2d(128, 64, 3)\n",
    "\n",
    "        self.convtrans7_1 = nn.ConvTranspose2d(64, 2, 2)\n",
    "        self.convtrans7_2 = nn.ConvTranspose2d(64, 2, 2)\n",
    "\n",
    "        self.convtrans8_1 = nn.ConvTranspose2d(2, 2, 3)\n",
    "        self.convtrans8_2 = nn.ConvTranspose2d(2, 2, 3)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        #print(x1.shape, x2.shape)\n",
    "        # x1, x2 shape: (896, 384)      channel = 1\n",
    "        x1_origin = x1\n",
    "        x2_origin = x2\n",
    "\n",
    "        x1 = self.pool(F.relu(self.conv1_1(x1)))    # shape: (448, 384)->(446, 382)->(223, 191)\n",
    "\n",
    "        x1 = self.pool(F.relu(self.conv2_1(x1)))    # shape: (223, 191)->(221, 189)->(111, 95)\n",
    "\n",
    "        x1 = F.relu(self.conv3_1(x1))               # shape: (111, 95)->(109, 93)\n",
    "        x1 = self.pool(F.relu(self.conv4_1(x1)))    # shape: (109, 93)->(107, 91)->(54, 46)\n",
    "\n",
    "        x1 = F.relu(self.conv5_1(x1))               # shape: (54, 46)->(52, 44)\n",
    "        x1 = self.pool(F.relu(self.conv6_1(x1)))    # shape: (52, 44)->(50, 42)->(25, 21)\n",
    "\n",
    "        x1 = F.relu(self.conv7_1(x1))               # shape: (25, 21)->(23, 19)\n",
    "        x1 = self.pool(F.relu(self.conv8_1(x1)))    # shape: (23, 19)->(21, 17)->(11, 9)\n",
    "\n",
    "\n",
    "        x2 = self.pool(F.relu(self.conv1_2(x2)))    # shape: (448, 384)->(446, 382)->(223, 191)\n",
    "        \n",
    "        x2 = self.pool(F.relu(self.conv2_2(x2)))    # shape: (223, 191)->(221, 189)->(111, 95)\n",
    "\n",
    "        x2 = F.relu(self.conv3_2(x2))               # shape: (111, 95)->(109, 93)\n",
    "        x2 = self.pool(F.relu(self.conv4_2(x2)))    # shape: (109, 93)->(107, 91)->(54, 46)\n",
    "\n",
    "        x2 = F.relu(self.conv5_2(x2))               # shape: (54, 46)->(52, 44)\n",
    "        x2 = self.pool(F.relu(self.conv6_2(x2)))    # shape: (52, 44)->(50, 42)->(25, 21)\n",
    "\n",
    "        x2 = F.relu(self.conv7_2(x2))               # shape: (25, 21)->(23, 19)\n",
    "        x2 = self.pool(F.relu(self.conv8_2(x2)))    # shape: (23, 19)->(21, 17)->(11, 9)\n",
    "\n",
    "        x_all = torch.cat((x1, x2), 1)\n",
    "\n",
    "        x1reco = self.unpool(F.relu(self.convtrans1_1(x_all)))    # shape: (11, 9)->(24, 20)\n",
    "        x1reco = F.relu(self.convtrans2_1(x1reco))    # shape: (24, 20)->(25, 21)\n",
    "\n",
    "        x1reco = self.unpool(F.relu(self.convtrans3_1(x1reco)))    # shape: (25, 21)->(52, 44)\n",
    "        x1reco = F.relu(self.convtrans4_1(x1reco))    # shape: (52, 44)->(53, 45)\n",
    "\n",
    "        x1reco = self.unpool(F.relu(self.convtrans5_1(x1reco)))    # shape: (53, 45)->(108, 92)\n",
    "        x1reco = F.relu(self.convtrans6_1(x1reco))    # shape: (108, 92)->(109, 93)\n",
    "\n",
    "        x1reco = self.unpool(F.relu(self.convtrans7_1(x1reco)))    # shape: (109, 93)->(220, 188)\n",
    "        x1reco = self.unpool(F.relu(self.convtrans8_1(x1reco)))    # shape: (220, 188)->(442, 378)\n",
    "\n",
    "\n",
    "        x2reco = self.unpool(F.relu(self.convtrans1_2(x_all)))    # shape: (11, 9)->(24, 20)\n",
    "        x2reco = F.relu(self.convtrans2_2(x2reco))    # shape: (24, 20)->(25, 21)\n",
    "\n",
    "        x2reco = self.unpool(F.relu(self.convtrans3_2(x2reco)))    # shape: (25, 21)->(52, 44)\n",
    "        x2reco = F.relu(self.convtrans4_2(x2reco))    # shape: (52, 44)->(53, 45)\n",
    "\n",
    "        x2reco = self.unpool(F.relu(self.convtrans5_2(x2reco)))    # shape: (53, 45)->(108, 92)\n",
    "        x2reco = F.relu(self.convtrans6_2(x2reco))    # shape: (108, 92)->(109, 93)\n",
    "\n",
    "        x2reco = self.unpool(F.relu(self.convtrans7_2(x2reco)))    # shape: (109, 93)->(220, 188)\n",
    "        x2reco = self.unpool(F.relu(self.convtrans8_2(x2reco)))    # shape: (220, 188)->(442, 378)\n",
    "        \n",
    "        x1reco=torch.squeeze(x1reco)\n",
    "        x2reco=torch.squeeze(x2reco)\n",
    "        #print(x1reco.shape, x2reco.shape)\n",
    "        return x1reco+x1_origin, x2reco+x2_origin\n",
    "net=Net().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\swyx2\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 448, 384])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aout, _=net.forward(mydataset.__getitem__(0)[0][None, :, :, :].cuda(), mydataset.__getitem__(0)[1][None, :, :, :].cuda())\n",
    "aout.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criterion(aout, mydataset.__getitem__(0)[2][None, :, :].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 448, 384])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aout.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.Tensor([1, 200]).cuda())\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26718288"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce GTX 1660 Ti'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, batch: 0 Loss: 0.744705\n",
      "Epoch: 0, batch: 1 Loss: 0.459503\n",
      "Epoch: 0, batch: 2 Loss: 0.459997\n",
      "Epoch: 0, batch: 3 Loss: 0.458179\n",
      "Epoch: 0, batch: 4 Loss: 0.194114\n",
      "Epoch: 0, batch: 5 Loss: 0.856371\n",
      "Epoch: 0, batch: 6 Loss: 0.193540\n",
      "Epoch: 0, batch: 7 Loss: 1.397960\n",
      "Epoch: 0, batch: 8 Loss: 0.193219\n",
      "Epoch: 0, batch: 9 Loss: 0.406471\n",
      "Epoch: 0, batch: 10 Loss: 0.192941\n",
      "Epoch: 0, batch: 11 Loss: 0.192420\n",
      "Epoch: 0, batch: 12 Loss: 0.192286\n",
      "Epoch: 0, batch: 13 Loss: 0.191459\n",
      "Epoch: 0, batch: 14 Loss: 0.190925\n",
      "Epoch: 0, batch: 15 Loss: 0.190265\n",
      "Epoch: 0, batch: 16 Loss: 0.189437\n",
      "Epoch: 0, batch: 17 Loss: 1.114248\n",
      "Epoch: 0, batch: 18 Loss: 0.188158\n",
      "Epoch: 0, batch: 19 Loss: 0.716238\n",
      "Epoch: 0, batch: 20 Loss: 0.476771\n",
      "Epoch: 0, batch: 21 Loss: 0.448384\n",
      "Epoch: 0, batch: 22 Loss: 0.185702\n",
      "Epoch: 0, batch: 23 Loss: 0.390647\n",
      "Epoch: 0, batch: 24 Loss: 0.184443\n",
      "Epoch: 0, batch: 25 Loss: 0.183722\n",
      "Epoch: 0, batch: 26 Loss: 0.467227\n",
      "Epoch: 0, batch: 27 Loss: 0.877220\n",
      "Epoch: 0, batch: 28 Loss: 0.943082\n",
      "Epoch: 0, batch: 29 Loss: 0.735362\n",
      "Epoch: 0, batch: 30 Loss: 0.738616\n",
      "Epoch: 0, batch: 31 Loss: 0.180994\n",
      "Epoch: 0, batch: 32 Loss: 0.453022\n",
      "Epoch: 0, batch: 33 Loss: 0.674231\n",
      "Epoch: 0, batch: 34 Loss: 1.109339\n",
      "Epoch: 0, batch: 35 Loss: 0.179870\n",
      "Epoch: 0, batch: 36 Loss: 0.717716\n",
      "Epoch: 0, batch: 37 Loss: 0.922337\n",
      "Epoch: 0, batch: 38 Loss: 1.915016\n",
      "Epoch: 0, batch: 39 Loss: 0.180013\n",
      "Epoch: 0, batch: 40 Loss: 1.147750\n",
      "Epoch: 0, batch: 41 Loss: 0.180753\n",
      "Epoch: 0, batch: 42 Loss: 0.180765\n",
      "Epoch: 0, batch: 43 Loss: 0.180982\n",
      "Epoch: 0, batch: 44 Loss: 0.699392\n",
      "Epoch: 0, batch: 45 Loss: 0.717481\n",
      "Epoch: 0, batch: 46 Loss: 0.180857\n",
      "Epoch: 0, batch: 47 Loss: 1.322124\n",
      "Epoch: 0, batch: 48 Loss: 0.729201\n",
      "Epoch: 0, batch: 49 Loss: 0.181150\n",
      "Epoch: 0, batch: 50 Loss: 0.181264\n",
      "Epoch: 0, batch: 51 Loss: 0.442550\n",
      "Epoch: 0, batch: 52 Loss: 0.706529\n",
      "Epoch: 0, batch: 53 Loss: 0.470323\n",
      "Epoch: 0, batch: 54 Loss: 0.677801\n",
      "Epoch: 0, batch: 55 Loss: 1.028053\n",
      "Epoch: 0, batch: 56 Loss: 0.837543\n",
      "Epoch: 0, batch: 57 Loss: 0.180863\n",
      "Epoch: 0, batch: 58 Loss: 0.180981\n",
      "Epoch: 0, batch: 59 Loss: 0.180559\n",
      "Epoch: 0, batch: 60 Loss: 0.180480\n",
      "Epoch: 0, batch: 61 Loss: 0.899327\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-1ba6c70a4389>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m50\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[1;31m#print(\"Epoch: {}, batch: {} Loss: {} label_loss:{}\".format(i, batch_idx, loss, label_loss_))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Epoch: {}, batch: {} Loss: {:0.6f}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# begin testing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36m__format__\u001b[1;34m(self, format_spec)\u001b[0m\n\u001b[0;32m    558\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__format__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat_spec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__format__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformat_spec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    561\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__format__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat_spec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training\n",
    "loss_list = []\n",
    "alpha=0.2\n",
    "epochs = 100\n",
    "\n",
    "\n",
    "accuracy_list = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    net.train()\n",
    "    for (batch_idx, batch) in enumerate(trainloader):\n",
    "        XZ_train_batch = batch[0].cuda() # remove .cuda() if you don't have a GPU\n",
    "        YZ_train_batch = batch[1].cuda() # remove .cuda() if you don't have a GPU\n",
    "        labelmap_xz_train_batch = batch[2].cuda() # remove .cuda() if you don't have a GPU\n",
    "        labelmap_yz_train_batch = batch[3].cuda() # remove .cuda() if you don't have a GPU\n",
    "\n",
    "        Netoutxz, Netoutyz = net.forward(XZ_train_batch, YZ_train_batch) # This will call the forward function, usually it returns tensors.\n",
    "        #print(F.softmax(Netout))\n",
    "        #print(Netoutxz.shape, Netoutyz.shape)\n",
    "        #print(labelmap_xz_train_batch.shape, labelmap_yz_train_batch.shape)\n",
    "\n",
    "        loss = criterion(Netoutxz, labelmap_xz_train_batch)+criterion(Netoutyz, labelmap_yz_train_batch) # classification loss\n",
    "        \n",
    "        # Zero the gradients before running the backward pass.\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "        # parameters of the model. Internally, the parameters of each Module are stored\n",
    "        # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "        # all learnable parameters in the model.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Calling the step function on an Optimizer makes an update to its\n",
    "        # parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_list.append(loss)\n",
    "        if batch_idx % 50 == 0 or True:\n",
    "            #print(\"Epoch: {}, batch: {} Loss: {} label_loss:{}\".format(i, batch_idx, loss, label_loss_))\n",
    "            print(\"Epoch: {}, batch: {} Loss: {:0.6f}\".format(i, batch_idx, loss))\n",
    "    \n",
    "    net.eval() # begin testing\n",
    "    preds = np.array([])\n",
    "    reals = np.array([])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (batch_idx, batch) in enumerate(testloader):\n",
    "            XZ_test_batch = batch[0].cuda() # remove .cuda() if you don't have a GPU\n",
    "            YZ_test_batch = batch[1].cuda() # remove .cuda() if you don't have a GPU\n",
    "            labelmap_xz_test_batch = batch[2].cuda() # remove .cuda() if you don't have a GPU\n",
    "            labelmap_yz_test_batch = batch[3].cuda() # remove .cuda() if you don't have a GPU\n",
    "\n",
    "\n",
    "            Netoutxz_, Netoutyz_ = net.forward(XZ_test_batch, YZ_test_batch) # This will call the forward function, usually it returns tensors.\n",
    "\n",
    "\n",
    "            predictionxz=F.softmax(Netoutxz, dim=1).cpu().numpy()\n",
    "            predictionyz=F.softmax(Netoutyz, dim=1).cpu().numpy()\n",
    "            \n",
    "            preds=np.concatenate((preds, np.sum(predictionxz, axis=(1,2,3) )))\n",
    "            reals=np.concatenate((reals, np.sum(predictionyz, axis=(1,2,3) )))\n",
    "        preds=np.array(preds)\n",
    "        reals=np.array(reals)\n",
    "        accuracy=np.sum((preds-reals)**2)\n",
    "        accuracy_list.append(accuracy)\n",
    "        print(\"Squared Error: {}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_listnp=[i.cpu().detach().numpy() for i in loss_list]\n",
    "plt.plot(loss_listnp)\n",
    "plt.ylim(0, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-338f5d0640b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mNetoutxz_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNetoutyz_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXZ_test_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYZ_test_batch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# This will call the forward function, usually it returns tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNetoutxz_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabelmap_xz_test_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNetoutyz_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabelmap_yz_test_batch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# classification loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-62b25071cf98>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x1, x2)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[0mx2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1_2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m    \u001b[1;31m# shape: (448, 384)->(446, 382)->(223, 191)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m         \u001b[0mx2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2_2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m    \u001b[1;31m# shape: (223, 191)->(221, 189)->(111, 95)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[0mx2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv3_2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m               \u001b[1;31m# shape: (111, 95)->(109, 93)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 443\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    437\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m--> 439\u001b[1;33m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    440\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net.eval() # begin testing\n",
    "preds = np.array([])\n",
    "reals = np.array([])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for (batch_idx, batch) in enumerate(testloader):\n",
    "        XZ_test_batch = batch[0].cuda() # remove .cuda() if you don't have a GPU\n",
    "        YZ_test_batch = batch[1].cuda() # remove .cuda() if you don't have a GPU\n",
    "        labelmap_xz_test_batch = batch[2].cuda() # remove .cuda() if you don't have a GPU\n",
    "        labelmap_yz_test_batch = batch[3].cuda() # remove .cuda() if you don't have a GPU\n",
    "\n",
    "\n",
    "        Netoutxz_, Netoutyz_ = net.forward(XZ_test_batch, YZ_test_batch) # This will call the forward function, usually it returns tensors.\n",
    "\n",
    "        loss = criterion(Netoutxz_, labelmap_xz_test_batch)+criterion(Netoutyz_, labelmap_yz_test_batch) # classification loss\n",
    "\n",
    "        predictionxz=F.softmax(Netoutxz, dim=1).cpu().numpy()\n",
    "        realxz=labelmap_xz_test_batch.cpu().numpy()\n",
    "        \n",
    "        preds=np.concatenate((preds, np.sum(predictionxz, axis=(1,2,3) )))\n",
    "        reals=np.concatenate((reals, np.sum(realxz, axis=(1,2) )))\n",
    "    preds=np.array(preds)\n",
    "    reals=np.array(reals)\n",
    "    accuracy=np.sum((preds-reals)**2)\n",
    "    accuracy_list.append(accuracy)\n",
    "    print(\"Squared Error: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "482"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=predictionyz[0, 1, :, :]\n",
    "np.count_nonzero(b>0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0.,  903.,    0.,    0.,  441.,  441.,    0.,  441.,    0.,\n",
       "        441.,  462.,    0.,  441.,    0.,    0.,    0.,    0.,  441.,\n",
       "          0.,    0.,    0.,    0.,  903.,  966.,    0.,    0.,    0.,\n",
       "        441.,  441.,  441.,    0.,    0.,    0., 1323.,    0.,    0.,\n",
       "        672.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,  483.,    0.,  441.,    0.,    0.,    0.,\n",
       "          0.,  441.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,  462.,    0.,    0.,    0.,\n",
       "        483.,  441.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0., 1513.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0., 2226.,    0.,    0.,    0.,    0.,  231.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,  441.,    0.,    0.,    0.,  441.,\n",
       "          0.,  441.,    0.,    0.,  882.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,  441.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0., 1950.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,  441.,    0.,  441.,    0.,    0.,    0.,    0.,    0.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ,\n",
       "       172031.953125, 172031.9375  , 172031.984375, 172031.9375  ])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 448, 384])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelmap_xz_train_batch[:,:,:].to(torch.float).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 448, 384])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Netoutxz[:, 1, :, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171518"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(predictionxz[0,0,:,:] > 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-443.36954, -119.60993,  223.00009,  353.92197], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(XZ_train_batch.cpu().numpy(), axis=(1,2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siglist=[]\n",
    "for items in file_list:\n",
    "    data = np.load( os.path.join( data_path, items ) )\n",
    "    siglist.append([items, data['sig']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siglist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "preds = []\n",
    "reals = []\n",
    "\n",
    "for (batch_idx, batch) in enumerate(testloader):\n",
    "    XZ_test_batch = batch[0].cuda() # remove .cuda() if you don't have a GPU\n",
    "    YZ_test_batch = batch[1].cuda() # remove .cuda() if you don't have a GPU\n",
    "    sig_test_batch = batch[2].cuda() # remove .cuda() if you don't have a GPU\n",
    "\n",
    "    Netout = net.forward(XZ_test_batch, YZ_test_batch) # This will call the forward function, usually it returns tensors.\n",
    "    #print(Netout.shape)\n",
    "    prediction=Netout\n",
    "    #print(predictor)\n",
    "\n",
    "    preds.append(prediction.cpu().detach().numpy())\n",
    "    \n",
    "    reals.append(sig_test_batch.cpu().detach().numpy())\n",
    "preds=np.array(preds)\n",
    "reals=np.array(reals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_cm = pd.DataFrame(confusion_matrix(reals.flatten(), preds.flatten()), index = ['background', 'signal'],\n",
    "                  columns = ['background', 'signal'])\n",
    "plt.figure(figsize = (10,7))\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "sn.heatmap(df_cm, annot=True, cmap=\"jet\")\n",
    "plt.show()\n",
    "\n",
    "# Confusion matrix whose i-th row and j-th column entry indicates the number of samples with true label being i-th class and predicted label being j-th class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "359"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(reals.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net, './model_save_train_VGG_reduced_time.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.load('./model_save_train_VGG_reduced_time.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 45,  12],\n",
       "       [  8, 335]], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(reals.flatten(), preds.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6e343a66b0d3f3fb9e5b3006acd45e89d57a985b4e0912ddff9600a29bb2e852"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
